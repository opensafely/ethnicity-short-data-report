{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ CONFIGURE OPTIONS HERE ################################\n",
    "\n",
    "# Import file\n",
    "input_path = \"C:/Users/candrews/Documents/GitHub/ethnicity-short-data-report/output/data/input.feather\"\n",
    "\n",
    "# Definitions\n",
    "definitions = [\"ethnicity_5\", \"ethnicity_new_5\", \"ethnicity_primis_5\"]\n",
    "\n",
    "# Code dictionary\n",
    "code_dict = {\n",
    "    \"imd\": {\n",
    "        0: \"Unknown\",\n",
    "        1: \"1 Most deprived\",\n",
    "        2: \"2\",\n",
    "        3: \"3\",\n",
    "        4: \"4\",\n",
    "        5: \"5 Least deprived\",\n",
    "    },\n",
    "    \"ethnicity_5\": {1: \"White\", 2: \"Mixed\", 3: \"Asian\", 4: \"Black\", 5: \"Other\"},\n",
    "    \"ethnicity_new_5\": {1: \"White\", 2: \"Mixed\", 3: \"Asian\", 4: \"Black\", 5: \"Other\"},\n",
    "    \"ethnicity_primis_5\": {1: \"White\", 2: \"Mixed\", 3: \"Asian\", 4: \"Black\", 5: \"Other\"},\n",
    "}\n",
    "\n",
    "# Other variables to include\n",
    "other_vars = [\"white\", \"mixed\", \"asian\", \"black\", \"other\"]\n",
    "other_vars_combined = [x + \"_\" + y for x in definitions for y in other_vars]\n",
    "\n",
    "# Restrict to registered as of index date\n",
    "registered = True\n",
    "reg = \"registered\"\n",
    "\n",
    "eth_dates = True\n",
    "# Dates\n",
    "dates = False\n",
    "date_min = \"\"\n",
    "date_max = \"\"\n",
    "time_delta = \"\"\n",
    "\n",
    "# Min/max range\n",
    "min_range = 4\n",
    "max_range = 200\n",
    "\n",
    "# Null value – could be multiple values in a list [0,'0',NA]\n",
    "null = [0, \"0\"]\n",
    "\n",
    "# Covariates\n",
    "demographic_covariates = [\"age_band\", \"sex\", \"region\", \"imd\"]\n",
    "clinical_covariates = [\"dementia\", \"diabetes\", \"hypertension\", \"learning_disability\"]\n",
    "\n",
    "# Output filepath\n",
    "output_path = \"phenotype_validation_ethnicity/5\"\n",
    "if registered == True:\n",
    "    output_path = output_path + \"/registered\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def redact_round_table(df_in):\n",
    "    \"\"\"Redacts counts <= 5 and rounds counts to nearest 5\"\"\"\n",
    "    df_out = df_in.where(df_in > 5, np.nan).apply(lambda x: 5 * round(x / 5))\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def import_clean(\n",
    "    input_path,\n",
    "    definitions,\n",
    "    other_vars,\n",
    "    demographic_covariates,\n",
    "    clinical_covariates,\n",
    "    reg,\n",
    "    null,\n",
    "    date_min,\n",
    "    date_max,\n",
    "    time_delta,\n",
    "    output_path,\n",
    "    code_dict=\"\",\n",
    "    dates=False,\n",
    "    registered=True,\n",
    "    eth_dates = True,\n",
    "):\n",
    "    # Import\n",
    "    df_import = pd.read_feather(input_path)\n",
    "\n",
    "    # restrict to registered as of indes date\n",
    "    if registered == True:\n",
    "        df_import = df_import[df_import[reg]]\n",
    "\n",
    "    # Dates\n",
    "    if dates == True:\n",
    "        date_vars = [definition + \"_date\" for definition in definitions]\n",
    "        # Create variable that captures difference in measurement dates\n",
    "        date_diff_vars = []\n",
    "        # Define start and end dates\n",
    "        start_date = datetime.datetime.strptime(date_min, \"%Y-%m-%d\")\n",
    "        end_date = datetime.datetime.strptime(date_max, \"%Y-%m-%d\")\n",
    "        for definition in definitions:\n",
    "            # Remove OpenSAFELY null dates\n",
    "            df_import.loc[\n",
    "                df_import[definition + \"_date\"] == \"1900-01-01\", definition + \"_date\"\n",
    "            ] = np.nan\n",
    "            # Limit to period of interest\n",
    "            df_import[definition + \"_date\"] = pd.to_datetime(\n",
    "                df_import[definition + \"_date\"]\n",
    "            )\n",
    "            df_import.loc[\n",
    "                df_import[definition + \"_date\"] < start_date, definition + \"_date\"\n",
    "            ] = np.nan\n",
    "            df_import.loc[\n",
    "                df_import[definition + \"_date\"] > end_date, definition + \"_date\"\n",
    "            ] = np.nan\n",
    "            # Remove the measurement if outside the date parameters\n",
    "            df_import.loc[df_import[definition + \"_date\"].isna(), definition] = np.nan\n",
    "            df_import\n",
    "            # Create difference between measurement dates\n",
    "            df_import[definition + \"_date\"] = (\n",
    "                df_import[definition + \"_date\"]\n",
    "                .dt.to_period(time_delta)\n",
    "                .dt.to_timestamp()\n",
    "            )\n",
    "            df_import = df_import.sort_values(by=[\"patient_id\", definition + \"_date\"])\n",
    "            df_import[\"date_diff_\" + definition] = round(\n",
    "                df_import.groupby(\"patient_id\")[definition + \"_date\"].diff()\n",
    "                / np.timedelta64(1, time_delta)\n",
    "            )\n",
    "            date_diff_vars.append(\"date_diff_\" + definition)\n",
    "    else:\n",
    "        date_vars = []\n",
    "        date_diff_vars = []\n",
    "    # Codes\n",
    "    if code_dict != \"\":\n",
    "        for key in code_dict:\n",
    "            df_import[key] = df_import[key].astype(float)\n",
    "            df_import[key] = df_import[key].replace(code_dict[key])\n",
    "    # Subset to relevant columns\n",
    "    if eth_dates:\n",
    "        ethnicity_dates=[\n",
    "            f\"{definition}_date\"\n",
    "            for definition in definitions\n",
    "        ]\n",
    "    df_clean = df_import[\n",
    "        [\"patient_id\"]\n",
    "        + definitions\n",
    "        + other_vars\n",
    "        + date_vars\n",
    "        + date_diff_vars\n",
    "        + demographic_covariates\n",
    "        + clinical_covariates\n",
    "        + ethnicity_dates\n",
    "    ]\n",
    "\n",
    "    # Limit to relevant date range\n",
    "    df_clean = df_clean.sort_values(by=\"patient_id\").reset_index(drop=True)\n",
    "    # Set null values to nan\n",
    "    for definition in definitions:\n",
    "        df_clean.loc[df_clean[definition].isin(null), definition] = np.nan\n",
    "    # Create order for categorical variables\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        if df_clean[group].dtype.name == \"category\":\n",
    "            li_order = sorted(df_clean[group].dropna().unique().tolist())\n",
    "            df_clean[group] = df_clean[group].cat.reorder_categories(\n",
    "                li_order, ordered=True\n",
    "            )\n",
    "    # Mark patients with value filled/missing for each definition\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_fill = pd.DataFrame(\n",
    "            df_clean.groupby(\"patient_id\")[definition].any().astype(\"int\")\n",
    "        ).rename(columns={definition: definition + \"_filled\"})\n",
    "        df_fill[definition + \"_missing\"] = 1 - df_fill[definition + \"_filled\"]\n",
    "        li_filled.append(df_fill)\n",
    "\n",
    "    df_filled = pd.concat(li_filled, axis=1)\n",
    "    # Remove list from memory\n",
    "    del li_filled\n",
    "    df_clean = df_clean.merge(df_filled, on=\"patient_id\")\n",
    "\n",
    "    # Flag all filled/all missing\n",
    "    li_col_filled = [col for col in df_clean.columns if col.endswith(\"_filled\")]\n",
    "    li_col_missing = [col for col in df_clean.columns if col.endswith(\"_missing\")]\n",
    "    df_clean[\"all_filled\"] = (\n",
    "        df_clean[li_col_filled].sum(axis=1) == len(definitions)\n",
    "    ).astype(int)\n",
    "    df_clean[\"all_missing\"] = (\n",
    "        df_clean[li_col_missing].sum(axis=1) == len(definitions)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Check whether output paths exist or not, create if missing\n",
    "    path_tables = f\"output/{output_path}/tables\"\n",
    "    path_figures = f\"output/{output_path}/figures\"\n",
    "    li_filepaths = [path_tables, path_figures]\n",
    "\n",
    "    for filepath in li_filepaths:\n",
    "        exists = os.path.exists(filepath)\n",
    "        if not exists:\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def patient_counts(\n",
    "    df_clean,\n",
    "    definitions,\n",
    "    demographic_covariates,\n",
    "    clinical_covariates,\n",
    "    output_path,\n",
    "    code_dict=\"\",\n",
    "    categories=False,\n",
    "    missing=False,\n",
    "):\n",
    "    suffix = \"_filled\"\n",
    "    subgroup = \"with records\"\n",
    "    overlap = \"all_filled\"\n",
    "    if missing == True:\n",
    "        suffix = \"_missing\"\n",
    "        subgroup = \"missing records\"\n",
    "        overlap = \"all_missing\"\n",
    "    if categories == True:\n",
    "        li_cat_def = []\n",
    "        li_cat = (\n",
    "            df_clean[definitions[0]]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .sort_values()\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        for x in li_cat:\n",
    "            for definition in definitions:\n",
    "                df_clean.loc[df_clean[definition] == x, f\"{x}_{definition}_filled\"] = 1\n",
    "                li_cat_def.append(f\"-{x}-{definition}\")\n",
    "        if code_dict != \"\":\n",
    "            for i in code_dict[definition]:\n",
    "                li_cat_def = list(\n",
    "                    map(\n",
    "                        lambda x: x.replace(code_dict[definition][i], str(i)),\n",
    "                        li_cat_def,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            li_cat_def = sorted(li_cat_def)\n",
    "            for i in code_dict[definition]:\n",
    "                li_cat_def = list(\n",
    "                    map(\n",
    "                        lambda x: x.replace(\n",
    "                            f\"-{str(i)}-\", f\"{code_dict[definition][i]}_\"\n",
    "                        ),\n",
    "                        li_cat_def,\n",
    "                    )\n",
    "                )\n",
    "        definitions = li_cat_def\n",
    "\n",
    "    # All with measurement\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_temp = (\n",
    "            df_clean[[\"patient_id\", definition + suffix]]\n",
    "            .drop_duplicates()\n",
    "            .dropna()\n",
    "            .set_index(\"patient_id\")\n",
    "        )\n",
    "        li_filled.append(df_temp)\n",
    "\n",
    "    df_temp = (\n",
    "        df_clean[[\"patient_id\", overlap]]\n",
    "        .drop_duplicates()\n",
    "        .dropna()\n",
    "        .set_index(\"patient_id\")\n",
    "    )\n",
    "    li_filled.append(df_temp)\n",
    "\n",
    "    df_temp2 = pd.concat(li_filled, axis=1)\n",
    "    df_temp2[\"population\"] = 1\n",
    "    # Remove list from memory\n",
    "    del li_filled\n",
    "    df_all = pd.DataFrame(df_temp2.sum()).T\n",
    "    df_all[\"group\"], df_all[\"subgroup\"] = [\"all\", subgroup]\n",
    "    df_all = df_all.set_index([\"group\", \"subgroup\"])\n",
    "\n",
    "    # By group\n",
    "    li_group = []\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        li_filled_group = []\n",
    "        for definition in definitions:\n",
    "            df_temp = (\n",
    "                df_clean[[\"patient_id\", definition + suffix, group]]\n",
    "                .drop_duplicates()\n",
    "                .dropna()\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            li_filled_group.append(df_temp)\n",
    "\n",
    "        df_temp = (\n",
    "            df_clean[[\"patient_id\", overlap, group]]\n",
    "            .drop_duplicates()\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        li_filled_group.append(df_temp)\n",
    "\n",
    "        df_reduce = reduce(\n",
    "            lambda df1, df2: pd.merge(df1, df2, on=[\"patient_id\", group], how=\"outer\"),\n",
    "            li_filled_group,\n",
    "        )\n",
    "        df_reduce[\"population\"] = 1\n",
    "        # Remove list from memory\n",
    "        del li_filled_group\n",
    "        df_reduce2 = (\n",
    "            df_reduce.sort_values(by=group)\n",
    "            .drop(columns=[\"patient_id\"])\n",
    "            .groupby(group)\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        df_reduce2[\"group\"] = group\n",
    "        df_reduce2 = df_reduce2.rename(columns={group: \"subgroup\"})\n",
    "        li_group.append(df_reduce2)\n",
    "    df_all_group = pd.concat(li_group, axis=0, ignore_index=True).set_index(\n",
    "        [\"group\", \"subgroup\"]\n",
    "    )\n",
    "    # Remove list from memory\n",
    "    del li_group\n",
    "\n",
    "    # Redact\n",
    "    df_append = redact_round_table(df_all.append(df_all_group))\n",
    "\n",
    "    # Create percentage columns\n",
    "    for definition in definitions:\n",
    "        df_append[definition + \"_pct\"] = round(\n",
    "            (df_append[definition + suffix].div(df_append[\"population\"])) * 100, 1\n",
    "        )\n",
    "    df_append[overlap + \"_pct\"] = round(\n",
    "        (df_append[overlap].div(df_append[\"population\"])) * 100, 1\n",
    "    )\n",
    "\n",
    "    # Final redaction step\n",
    "    df_append = df_append.where(~df_append.isna(), \"-\")\n",
    "\n",
    "    # Combine count and percentage columns\n",
    "    for definition in definitions:\n",
    "        df_append[definition] = (\n",
    "            df_append[definition + suffix].astype(str)\n",
    "            + \" (\"\n",
    "            + df_append[definition + \"_pct\"].astype(str)\n",
    "            + \")\"\n",
    "        )\n",
    "        df_append = df_append.drop(columns=[definition + suffix, definition + \"_pct\"])\n",
    "    df_append[overlap] = (\n",
    "        df_append[overlap].astype(str)\n",
    "        + \" (\"\n",
    "        + df_append[overlap + \"_pct\"].astype(str)\n",
    "        + \")\"\n",
    "    )\n",
    "    df_append = df_append.drop(columns=[overlap + \"_pct\"])\n",
    "\n",
    "    # Column order\n",
    "    li_col_order = []\n",
    "    for definition in definitions:\n",
    "        li_col_order.append(definition)\n",
    "    if eth_dates:\n",
    "        li_col_order.append(definition+\"_date\")\n",
    "    li_col_order.append(overlap)\n",
    "    li_col_order.append(\"population\")\n",
    "\n",
    "    df_all_redact = df_append[li_col_order]\n",
    "    df_all_redact.columns = df_all_redact.columns.str.replace(\"_\", \" \")\n",
    "    display(df_all_redact)\n",
    "    if categories == False:\n",
    "        df_all_redact.to_csv(f\"output/{output_path}/tables/patient_counts{suffix}.csv\")\n",
    "    if categories == True:\n",
    "        df_all_redact.to_csv(\n",
    "            f\"output/{output_path}/tables/patient_counts_by_categories{suffix}.csv\"\n",
    "        )\n",
    "\n",
    "\n",
    "def display_heatmap(df_clean, definitions, output_path):\n",
    "    # All with measurement\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_temp = df_clean[[\"patient_id\"]].drop_duplicates().set_index(\"patient_id\")\n",
    "        df_temp[definition + \"_filled\"] = 1\n",
    "        df_temp = (\n",
    "            df_clean[[\"patient_id\", definition + \"_filled\"]]\n",
    "            .drop_duplicates()\n",
    "            .dropna()\n",
    "            .set_index(\"patient_id\")\n",
    "        )\n",
    "        li_filled.append(df_temp)\n",
    "\n",
    "    # Prepare data for heatmap input\n",
    "    df_temp2 = pd.concat(li_filled, axis=1)\n",
    "    # Remove list from memory\n",
    "    del li_filled\n",
    "    df_transform = df_temp2.replace(np.nan, 0)\n",
    "    df_dot = redact_round_table(df_transform.T.dot(df_transform))\n",
    "\n",
    "    # Create mask to eliminate duplicates in heatmap\n",
    "    mask = np.triu(np.ones_like(df_dot))\n",
    "    np.fill_diagonal(mask[::1], 0)\n",
    "\n",
    "    # Draw the heatmap with the mask\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.heatmap(df_dot, annot=True, mask=mask, fmt=\"g\", cmap=\"YlGnBu\", vmin=0)\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"output/{output_path}/figures/heatmap.png\")\n",
    "\n",
    "\n",
    "def records_over_time(\n",
    "    df_clean, definitions, demographic_covariates, clinical_covariates, output_path\n",
    "):\n",
    "    li_df = []\n",
    "    for definition in definitions:\n",
    "        df_grouped = (\n",
    "            df_clean[[definition + \"_date\", definition]]\n",
    "            .groupby(definition + \"_date\")\n",
    "            .count()\n",
    "            .reset_index()\n",
    "            .rename(columns={definition + \"_date\": \"date\"})\n",
    "            .set_index(\"date\")\n",
    "        )\n",
    "        li_df.append(redact_round_table(df_grouped))\n",
    "    df_all_time = (\n",
    "        pd.concat(li_df)\n",
    "        .stack()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_1\": \"variable\", 0: \"value\"})\n",
    "    )\n",
    "    # Remove list from memory\n",
    "    del li_df\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    fig.autofmt_xdate()\n",
    "    sns.lineplot(\n",
    "        x=\"date\", y=\"value\", hue=\"variable\", data=df_all_time, ax=ax\n",
    "    ).set_title(\"New records by month\")\n",
    "    ax.legend().set_title(\"\")\n",
    "    plt.savefig(f\"output/{output_path}/figures/records_over_time.png\")\n",
    "\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        for definition in definitions:\n",
    "            df_grouped = (\n",
    "                df_clean[[definition + \"_date\", definition, group]]\n",
    "                .groupby([definition + \"_date\", group])\n",
    "                .count()\n",
    "                .reset_index()\n",
    "                .rename(columns={definition + \"_date\": \"date\"})\n",
    "                .set_index([\"date\", group])\n",
    "            )\n",
    "            df_time = redact_round_table(df_grouped).reset_index()\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            fig.autofmt_xdate()\n",
    "            sns.lineplot(\n",
    "                x=\"date\", y=definition, hue=group, data=df_time, ax=ax\n",
    "            ).set_title(f\"{definition} recorded by {group} and month\")\n",
    "            ax.legend().set_title(\"\")\n",
    "            plt.savefig(\n",
    "                f\"output/{output_path}/figures/records_over_time_{definition}_{group}.png\"\n",
    "            )\n",
    "\n",
    "\n",
    "def report_distribution(df_occ, definitions, num_definitions, output_path, group=\"\"):\n",
    "    \"\"\"\n",
    "    Plots histogram or boxplots of distribution\n",
    "    \"\"\"\n",
    "    if group == \"\":\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions:\n",
    "\n",
    "                avg_value = pd.DataFrame(df_occ[definition].agg([\"mean\", \"count\"]))\n",
    "                if avg_value.loc[\"count\"][0] > 6:\n",
    "                    avg_value.loc[\"count\"][0] = 5 * round(avg_value.loc[\"count\"][0] / 5)\n",
    "                    print(f\"Average {definition}:\\n\")\n",
    "                    # display(avg_value)\n",
    "                    avg_value.to_csv(\n",
    "                        f\"output/{output_path}/tables/avg_value_{definition}.csv\"\n",
    "                    )\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    hist_data = df_occ[definition].loc[~df_occ[definition].isna()]\n",
    "                    plt.hist(hist_data, bins=np.arange(min(hist_data), max(hist_data)))\n",
    "                    plt.title(\"Distribution of \" + definition)\n",
    "                    # plt.show()\n",
    "                    plt.savefig(f\"output/{output_path}/figures/distribution.png\")\n",
    "                else:\n",
    "                    print(\"Table and plot redacted due to low counts.\")\n",
    "\n",
    "        else:\n",
    "            df_bp = df_occ[definitions]\n",
    "            avg = pd.DataFrame(df_bp.mean(), columns=[\"mean\"])\n",
    "            ct = pd.DataFrame(df_bp.count(), columns=[\"count\"])\n",
    "            avg_value = avg.merge(ct, left_index=True, right_index=True)\n",
    "            # Redact and round values\n",
    "            avg_value[\"count\"] = (\n",
    "                avg_value[\"count\"]\n",
    "                .where(avg_value[\"count\"] > 5, np.nan)\n",
    "                .apply(lambda x: 5 * round(x / 5) if ~np.isnan(x) else x)\n",
    "            )\n",
    "            print(\"Averages:\\n\")\n",
    "            # display(avg_value)\n",
    "            avg_value.to_csv(f\"output/{output_path}/tables/avg_value.csv\")\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            sns.boxplot(data=df_bp, showfliers=False)\n",
    "            plt.title(\"Distributions of values\")\n",
    "            # plt.show()\n",
    "            plt.savefig(f\"output/{output_path}/figures/distribution.png\")\n",
    "    else:\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions:\n",
    "                df_bp = df_occ[[group] + [definition]]\n",
    "                avg_value = df_bp.groupby(group)[definition].agg([\"mean\", \"count\"])\n",
    "                # Redact and round values\n",
    "                avg_value[\"count\"] = (\n",
    "                    avg_value[\"count\"]\n",
    "                    .where(avg_value[\"count\"] > 5, np.nan)\n",
    "                    .apply(lambda x: 5 * round(x / 5) if ~np.isnan(x) else x)\n",
    "                )\n",
    "                avg_value.loc[avg_value[\"count\"].isna(), [\"count\", \"mean\"]] = [\"-\", \"-\"]\n",
    "                print(f\"Averages by {group}:\\n\")\n",
    "                # display(avg_value)\n",
    "                avg_value.to_csv(\n",
    "                    f\"output/{output_path}/tables/avg_value_{definition}_{group}.csv\"\n",
    "                )\n",
    "                null_index = avg_value[avg_value[\"count\"] == \"-\"].index.tolist()\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(\n",
    "                    x=group,\n",
    "                    y=definition,\n",
    "                    data=df_bp.loc[~df_bp[group].isin(null_index)],\n",
    "                    showfliers=False,\n",
    "                )\n",
    "                plt.title(f\"Distributions by {group}\")\n",
    "                # plt.show()\n",
    "                plt.savefig(f\"output/{output_path}/figures/distribution_{group}.png\")\n",
    "        else:\n",
    "            if df_occ[group].dtype == \"bool\":\n",
    "                df_occ[group] = df_occ[group].apply(lambda x: str(x))\n",
    "            df_occ = df_occ.loc[~df_occ[group].isna()]  # Drop nan categories\n",
    "            df_bp = df_occ[[group] + definitions]\n",
    "            avg = df_bp.groupby(group).mean().add_prefix(\"avg_\")\n",
    "            ct = df_bp.groupby(group).count().add_prefix(\"ct_\")\n",
    "            avg_value = avg.merge(ct, left_on=group, right_on=group)\n",
    "            for definition in definitions:\n",
    "                # Redact and round values\n",
    "                avg_value[\"ct_\" + definition] = (\n",
    "                    avg_value[\"ct_\" + definition]\n",
    "                    .where(avg_value[\"ct_\" + definition] > 5, np.nan)\n",
    "                    .apply(lambda x: 5 * round(x / 5) if ~np.isnan(x) else x)\n",
    "                )\n",
    "                avg_value.loc[\n",
    "                    avg_value[\"ct_\" + definition].isna(),\n",
    "                    [\"ct_\" + definition, \"avg_\" + definition],\n",
    "                ] = [\"-\", \"-\"]\n",
    "            print(f\"Averages by {group}:\\n\")\n",
    "            # display(avg_value)\n",
    "            avg_value.to_csv(f\"output/{output_path}/tables/avg_value_{group}.csv\")\n",
    "            for definition in definitions:\n",
    "                null_index = []\n",
    "                null_index = avg_value[\n",
    "                    avg_value[\"ct_\" + definition] == \"-\"\n",
    "                ].index.tolist()\n",
    "                df_bp.loc[df_bp[group].isin(null_index), definition] = np.nan\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            df_plot = df_bp.melt(id_vars=group, value_vars=definitions)\n",
    "            sns.boxplot(\n",
    "                x=group, y=\"value\", hue=\"variable\", data=df_plot, showfliers=False\n",
    "            )\n",
    "            plt.title(f\"Distributions by {group}\")\n",
    "            # plt.show()\n",
    "            plt.savefig(f\"output/{output_path}/figures/distribution_{group}.png\")\n",
    "\n",
    "\n",
    "def report_out_of_range(\n",
    "    df_occ,\n",
    "    definitions,\n",
    "    min_range,\n",
    "    max_range,\n",
    "    num_definitions,\n",
    "    null,\n",
    "    output_path,\n",
    "    group=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Reports number of measurements outside of defined range\n",
    "    \"\"\"\n",
    "\n",
    "    def q25(x):\n",
    "        return x.quantile(0.25)\n",
    "\n",
    "    def q75(x):\n",
    "        return x.quantile(0.75)\n",
    "\n",
    "    li_dfs = []\n",
    "\n",
    "    df_oor = df_occ\n",
    "    for definition in definitions:\n",
    "        df_oor.loc[\n",
    "            (df_oor[definition] < min_range) | (df_oor[definition] > max_range),\n",
    "            \"out_of_range_\" + definition,\n",
    "        ] = 1\n",
    "        # Make definitions null if not out of range or empty\n",
    "        df_oor[\"oor_\" + definition] = df_oor[definition]\n",
    "        df_oor.loc[\n",
    "            (df_oor[\"out_of_range_\" + definition] != 1)\n",
    "            | (df_oor[definition].isin(null)),\n",
    "            \"oor_\" + definition,\n",
    "        ] = np.nan\n",
    "        if group == \"\":\n",
    "            try:\n",
    "                df_out = df_oor.agg(\n",
    "                    count=(\"oor_\" + definition, \"count\"),\n",
    "                    mean=(\"oor_\" + definition, \"mean\"),\n",
    "                    pct25=(\"oor_\" + definition, q25),\n",
    "                    pct75=(\"oor_\" + definition, q75),\n",
    "                )\n",
    "            except:\n",
    "                df_out = pd.DataFrame(\n",
    "                    [\n",
    "                        [\"count\", 0],\n",
    "                        [\"mean\", np.nan],\n",
    "                        [\"pct25\", np.nan],\n",
    "                        [\"pct75\", np.nan],\n",
    "                    ],\n",
    "                    columns=[\"index\", \"oor_\" + definition],\n",
    "                ).set_index(\"index\")\n",
    "            if df_out.loc[\"count\"][\"oor_\" + definition] > 6:\n",
    "                df_out.loc[\"count\"][\"oor_\" + definition] = 5 * round(\n",
    "                    df_out.loc[\"count\"][\"oor_\" + definition] / 5\n",
    "                )\n",
    "            else:\n",
    "                df_out[\"oor_\" + definition] = \"-\"\n",
    "        else:\n",
    "            df_out = (\n",
    "                df_oor.groupby(group)[\"oor_\" + definition]\n",
    "                .agg(\n",
    "                    [\n",
    "                        (\"count\", \"count\"),\n",
    "                        (\"mean\", \"mean\"),\n",
    "                        (\"pct25\", q25),\n",
    "                        (\"pct75\", q75),\n",
    "                    ]\n",
    "                )\n",
    "                .add_suffix(\"_\" + definition)\n",
    "            )\n",
    "            df_out.loc[\n",
    "                df_out[\"count_\" + definition] > 5, \"count_\" + definition\n",
    "            ] = 5 * round(df_out[\"count_\" + definition] / 5)\n",
    "            df_out.loc[\n",
    "                df_out[\"count_\" + definition] < 6,\n",
    "                [\n",
    "                    \"count_\" + definition,\n",
    "                    \"mean_\" + definition,\n",
    "                    \"pct25_\" + definition,\n",
    "                    \"pct75_\" + definition,\n",
    "                ],\n",
    "            ] = [\"-\", \"-\", \"-\", \"-\"]\n",
    "        li_dfs.append(df_out)\n",
    "\n",
    "    if num_definitions == 1:\n",
    "        # display(df_out)\n",
    "        df_out.to_csv(f\"output/{output_path}/tables/out_of_range.csv\")\n",
    "        # Remove list from memory\n",
    "        del li_dfs\n",
    "        if group == \"\":\n",
    "            if df_out[\"oor_\" + definition][\"count\"] != \"-\":\n",
    "                df_plot = df_oor[\"oor_\" + definition]\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                plt.hist(df_plot)\n",
    "                plt.title(\"Distribution of out of range \" + definition)\n",
    "                # plt.show()\n",
    "                plt.savefig(f\"output/{output_path}/figures/out_of_range.png\")\n",
    "            else:\n",
    "                print(\"Plot redacted due to low counts.\")\n",
    "        else:\n",
    "            df_oor = df_oor.loc[~df_oor[group].isna()]\n",
    "            for definition in definitions:\n",
    "                null_index = df_out[df_out[\"count_\" + definition] == \"-\"].index.tolist()\n",
    "                df_oor.loc[df_oor[group].isin(null_index), \"oor_\" + definition] = np.nan\n",
    "                df_bp = df_oor[[group] + [\"oor_\" + definition]]\n",
    "                if df_bp[\"oor_\" + definition].sum() > 0:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    sns.boxplot(\n",
    "                        x=group, y=\"oor_\" + definition, data=df_bp, showfliers=False\n",
    "                    )\n",
    "                    plt.title(f\"Distribution of out of range values by {group}\")\n",
    "                    plt.show()\n",
    "                    plt.savefig(\n",
    "                        f\"output/{output_path}/figures/out_of_range_{group}.png\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Plot redacted due to low counts.\")\n",
    "    else:\n",
    "        df_merged = reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left, right, left_index=True, right_index=True\n",
    "            ),\n",
    "            li_dfs,\n",
    "        )\n",
    "        # Remove list from memory\n",
    "        del li_dfs\n",
    "        # display(df_merged)\n",
    "        if group == \"\":\n",
    "            df_merged.to_csv(f\"output/{output_path}/tables/out_of_range.csv\")\n",
    "            cols = [\"oor_\" + definition for definition in definitions]\n",
    "            df_bp = df_oor[cols]\n",
    "            if df_merged[\"oor_\" + definition][\"count\"] == \"-\":\n",
    "                df_bp[\"oor_\" + definition] = np.nan\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(data=df_bp, showfliers=False)\n",
    "                plt.title(\"Distribution of out of range values\")\n",
    "                # plt.show()\n",
    "                plt.savefig(f\"output/{output_path}/figures/out_of_range.png\")\n",
    "            except:\n",
    "                print(\"Plot redacted due to low counts.\")\n",
    "        else:\n",
    "            df_merged.to_csv(f\"output/{output_path}/tables/out_of_range_{group}.csv\")\n",
    "            df_oor = df_oor.loc[~df_oor[group].isna()]\n",
    "            for definition in definitions:\n",
    "                null_index = df_merged[\n",
    "                    df_merged[\"count_\" + definition] == \"-\"\n",
    "                ].index.tolist()\n",
    "                df_oor.loc[df_oor[group].isin(null_index), \"oor_\" + definition] = np.nan\n",
    "            if df_oor[group].dtype == \"bool\":\n",
    "                df_oor[group] = df_oor[group].apply(lambda x: str(x))\n",
    "            cols = [\"oor_\" + definition for definition in definitions]\n",
    "            df_bp = df_oor[[group] + cols]\n",
    "            df_plot = df_bp.melt(id_vars=group, value_vars=cols)\n",
    "            if df_plot[\"value\"].sum() > 0:\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(\n",
    "                    x=group, y=\"value\", hue=\"variable\", data=df_plot, showfliers=False\n",
    "                )\n",
    "                plt.title(f\"Distribution of out of range values by {group}\")\n",
    "                # plt.show()\n",
    "                plt.savefig(f\"output/{output_path}/figures/out_of_range_{group}.png\")\n",
    "            else:\n",
    "                print(\"Plot redacted due to low counts.\")\n",
    "\n",
    "\n",
    "def report_update_frequency(\n",
    "    df_occ, definitions, time_delta, num_definitions, output_path, group=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots histogram or boxplot of update frequency and reports average update frequency\n",
    "    \"\"\"\n",
    "    if group == \"\":\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions:\n",
    "                avg_update_freq = df_occ.agg(\n",
    "                    avg_diff=(f\"date_diff_{definition}\", \"mean\"),\n",
    "                    count=(f\"date_diff_{definition}\", \"count\"),\n",
    "                )\n",
    "                if avg_update_freq.loc[\"count\"][0] > 6:\n",
    "                    avg_update_freq.loc[\"count\"][0] = 5 * round(\n",
    "                        avg_update_freq.loc[\"count\"][0] / 5\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Average update frequency of {definition} by {time_delta}:\\n\"\n",
    "                    )\n",
    "                    # display(avg_update_freq)\n",
    "                    avg_update_freq.to_csv(\n",
    "                        f\"output/{output_path}/tables/avg_update_frequency_{definition}.csv\"\n",
    "                    )\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    plt.hist(df_occ[\"date_diff_\" + definition])\n",
    "                    plt.title(\"Update frequency of \" + definition + f\" by {time_delta}\")\n",
    "                    # plt.show()\n",
    "                    plt.savefig(\n",
    "                        f\"output/{output_path}/figures/avg_update_frequency_{definition}.png\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Table and plot redacted due to low counts.\")\n",
    "        else:\n",
    "            cols = [\"date_diff_\" + x for x in definitions]\n",
    "            df_bp = df_occ[cols]\n",
    "            avg_update = pd.DataFrame(df_bp.mean(), columns=[\"avg_diff\"])\n",
    "            ct_update = pd.DataFrame(df_bp.count(), columns=[\"count\"])\n",
    "            avg_update_freq = avg_update.merge(\n",
    "                ct_update, left_index=True, right_index=True\n",
    "            )\n",
    "            # Redact and round values\n",
    "            avg_update_freq[\"count\"] = (\n",
    "                avg_update_freq[\"count\"]\n",
    "                .where(avg_update_freq[\"count\"] > 5, np.nan)\n",
    "                .apply(lambda x: 5 * round(x / 5) if ~np.isnan(x) else x)\n",
    "            )\n",
    "            print(f\"Average update frequency by {time_delta}:\\n\")\n",
    "            # display(avg_update_freq)\n",
    "            avg_update_freq.to_csv(\n",
    "                f\"output/{output_path}/tables/avg_update_frequency.csv\"\n",
    "            )\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            null_index = avg_update_freq[avg_update_freq[\"count\"] == \"-\"].index.tolist()\n",
    "            sns.boxplot(data=df_bp.drop(columns=null_index), showfliers=False)\n",
    "            plt.title(f\"Update frequency by {time_delta}\")\n",
    "            # plt.show()\n",
    "            plt.savefig(f\"output/{output_path}/figures/avg_update_frequency.png\")\n",
    "\n",
    "    else:\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions:\n",
    "                df_bp = df_occ[[group] + [\"date_diff_\" + definition]]\n",
    "                avg_update_freq = (\n",
    "                    df_occ.groupby(group)\n",
    "                    .agg(\n",
    "                        avg_diff=(\"date_diff_\" + definition, \"mean\"),\n",
    "                        count=(\"date_diff_\" + definition, \"count\"),\n",
    "                    )\n",
    "                    .reset_index()\n",
    "                )\n",
    "                # Redact and round values\n",
    "                avg_update_freq[\"count\"] = (\n",
    "                    avg_update_freq[\"count\"]\n",
    "                    .where(avg_update_freq[\"count\"] > 5, np.nan)\n",
    "                    .apply(lambda x: 5 * round(x / 5) if ~np.isnan(x) else x)\n",
    "                )\n",
    "                avg_update_freq.loc[\n",
    "                    avg_update_freq[\"count\"].isna(), [\"count\", \"avg_diff\"]\n",
    "                ] = [\"-\", \"-\"]\n",
    "                print(f\"Average update frequency by {group} and {time_delta}:\\n\")\n",
    "                # display(avg_update_freq)\n",
    "                avg_update_freq.to_csv(\n",
    "                    f\"output/{output_path}/tables/avg_update_frequency_{definition}.csv\"\n",
    "                )\n",
    "                null_index = avg_update_freq[\n",
    "                    avg_update_freq[\"count\"] == \"-\"\n",
    "                ].index.tolist()\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(\n",
    "                    x=group,\n",
    "                    y=\"date_diff_\" + definition,\n",
    "                    data=df_bp.loc[~df_bp[group].isin(null_index)].sort_index(),\n",
    "                    showfliers=False,\n",
    "                )\n",
    "                plt.title(f\"Update frequency by {group} and {time_delta}\")\n",
    "                # plt.show()\n",
    "                plt.savefig(\n",
    "                    f\"output/{output_path}/figures/avg_update_frequency_{definition}.png\"\n",
    "                )\n",
    "        else:\n",
    "            if df_occ[group].dtype == \"bool\":\n",
    "                df_occ[group] = df_occ[group].apply(lambda x: str(x))\n",
    "            df_occ = df_occ.loc[~df_occ[group].isna()]  # Drop nan categories\n",
    "            cols = [\"date_diff_\" + x for x in definitions]\n",
    "            df_sub = df_occ[[group] + cols]\n",
    "            avg_update = df_sub.groupby(group).mean().add_prefix(\"avg_\")\n",
    "            ct_update = df_sub.groupby(group).count().add_prefix(\"ct_\")\n",
    "            avg_update_freq = avg_update.merge(\n",
    "                ct_update, left_on=group, right_on=group\n",
    "            ).sort_index()\n",
    "            for definition in definitions:\n",
    "                # Redact and round values\n",
    "                avg_update_freq[\"ct_date_diff_\" + definition] = (\n",
    "                    avg_update_freq[\"ct_date_diff_\" + definition]\n",
    "                    .where(avg_update_freq[\"ct_date_diff_\" + definition] > 5, np.nan)\n",
    "                    .apply(lambda x: 5 * round(x / 5) if ~np.isnan(x) else x)\n",
    "                )\n",
    "                avg_update_freq.loc[\n",
    "                    avg_update_freq[\"ct_date_diff_\" + definition].isna(),\n",
    "                    [\"ct_date_diff_\" + definition, \"avg_date_diff_\" + definition],\n",
    "                ] = [\"-\", \"-\"]\n",
    "            # Sort by index\n",
    "            print(f\"Average update frequencies by {time_delta}:\\n\")\n",
    "            # display(avg_update_freq)\n",
    "            avg_update_freq.to_csv(\n",
    "                f\"output/{output_path}/tables/avg_update_frequency_{group}.csv\"\n",
    "            )\n",
    "            for definition in definitions:\n",
    "                null_index = []\n",
    "                null_index = avg_update_freq[\n",
    "                    avg_update_freq[\"ct_date_diff_\" + definition] == \"-\"\n",
    "                ].index.tolist()\n",
    "                df_sub.loc[\n",
    "                    df_sub[group].isin(null_index), \"date_diff_\" + definition\n",
    "                ] = np.nan\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            df_plot = df_sub.melt(id_vars=group, value_vars=cols)\n",
    "            sns.boxplot(\n",
    "                x=group, y=\"value\", hue=\"variable\", data=df_plot, showfliers=False\n",
    "            )\n",
    "            plt.title(f\"Update frequencies by {group} and {time_delta}\")\n",
    "            # plt.show()\n",
    "            plt.savefig(\n",
    "                f\"output/{output_path}/figures/avg_update_frequency_{group}.png\"\n",
    "            )\n",
    "\n",
    "\n",
    "def latest_common_comparison(\n",
    "    df_clean, definitions, other_vars, output_path, code_dict=\"\"\n",
    "):\n",
    "    for definition in definitions:\n",
    "        vars = [s for s in other_vars if s.startswith(definition)]\n",
    "        df_subset = df_clean.loc[~df_clean[definition].isna()]\n",
    "        df_subset = df_subset[[definition] + vars].set_index(definition)\n",
    "        df_subset = df_subset.replace(0, np.nan)\n",
    "        df_subset2 = df_subset.where(df_subset.eq(df_subset.max(1), axis=0))\n",
    "        # add check for tied most common ethnicity\n",
    "        # check=df_subset2.count(axis=1)\n",
    "        # check = df_subset2.loc[check>1]\n",
    "        # display(check)\n",
    "        df_subset_3 = df_subset2.notnull().astype(\"int\").reset_index()\n",
    "        df_sum = redact_round_table(df_subset_3.groupby(definition).sum())\n",
    "        df_counts = pd.DataFrame(\n",
    "            np.diagonal(df_sum),\n",
    "            index=df_sum.index,\n",
    "            columns=[f\"matching (n={np.diagonal(df_sum).sum()})\"],\n",
    "        )\n",
    "\n",
    "        df_sum2 = df_sum.copy(deep=True)\n",
    "        np.fill_diagonal(df_sum2.values, 0)\n",
    "        df_diag = pd.DataFrame(\n",
    "            df_sum2.sum(axis=1),\n",
    "            columns=[f\"not_matching (n={df_sum2.sum(axis=1).sum()})\"],\n",
    "        )\n",
    "        df_out = df_counts.merge(df_diag, right_index=True, left_index=True)\n",
    "\n",
    "        # sort rows by category index\n",
    "        df_out = df_out.reindex(list(code_dict[definition].values()))\n",
    "\n",
    "        df_out = df_out.where(~df_out.isna(), \"-\")\n",
    "        df_out.to_csv(\n",
    "            f\"output/{output_path}/tables/latest_common_simple_{definition}.csv\"\n",
    "        )\n",
    "        df_sum = redact_round_table(df_subset_3.groupby(definition).sum())\n",
    "        # sort columns by category index\n",
    "        df_sum.columns = df_sum.columns.str.replace(definition + \"_\", \"\")\n",
    "        df_sum.columns = df_sum.columns.str.lower()\n",
    "        if code_dict != \"\":\n",
    "            lowerlist = [x.lower() for x in (list(code_dict[definition].values()))]\n",
    "            df_sum = df_sum[lowerlist]\n",
    "        else:\n",
    "            df_sum = df_sum.reindex(sorted(df_sum.columns), axis=1)\n",
    "        # sort rows by category index\n",
    "        df_sum = df_sum.reindex(list(code_dict[definition].values()))\n",
    "        df_sum.columns = df_sum.columns.str.replace(\"_\", \" \")\n",
    "        for col in df_sum.columns:\n",
    "            df_sum = df_sum.rename(columns={col: f\"{col} (n={df_sum[col].sum()})\"})\n",
    "        df_sum = df_sum.where(~df_sum.isna(), \"-\")\n",
    "        df_sum.to_csv(\n",
    "            f\"output/{output_path}/tables/latest_common_expanded_{definition}.csv\"\n",
    "        )\n",
    "\n",
    "\n",
    "def state_change(df_clean, definitions, other_vars, output_path, code_dict=\"\"):\n",
    "    for definition in definitions:\n",
    "        vars = [s for s in other_vars if s.startswith(definition)]\n",
    "        df_subset = (\n",
    "            df_clean[[definition] + vars]\n",
    "            .replace(0, np.nan)\n",
    "            .set_index(definition)\n",
    "            .reset_index()\n",
    "        )\n",
    "        df_subset[\"n\"] = 1\n",
    "        # Count\n",
    "        df_subset2 = df_subset.loc[~df_subset[definition].isna()]\n",
    "        df_subset3 = redact_round_table(\n",
    "            df_subset2.groupby(definition).count()\n",
    "        ).reset_index()\n",
    "        # Set index\n",
    "        ### arrange rows by category value\n",
    "        df_subset3 = df_subset3.set_index(definition)\n",
    "        df_subset3 = df_subset3.reindex(list(code_dict[definition].values()))\n",
    "        df_subset3 = df_subset3.reset_index()\n",
    "        df_subset3[\"index\"] = (\n",
    "            df_subset3[definition].astype(str)\n",
    "            + \" (n = \"\n",
    "            + df_subset3[\"n\"].astype(int).astype(str)\n",
    "            + \")\"\n",
    "        )\n",
    "        df_out = (\n",
    "            df_subset3.drop(columns=[definition, \"n\"])\n",
    "            .rename(columns={\"index\": definition})\n",
    "            .set_index(definition)\n",
    "        )\n",
    "        df_out.columns = df_out.columns.str.replace(definition + \"_\", \"\")\n",
    "        # rearrange columns by category value\n",
    "        df_out.columns = df_out.columns.str.lower()\n",
    "        if code_dict != \"\":\n",
    "            lowerlist = [x.lower() for x in (list(code_dict[definition].values()))]\n",
    "            df_out = df_out[lowerlist]\n",
    "        else:\n",
    "            df_out = df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "        # Null out the diagonal\n",
    "        # np.fill_diagonal(df_out.values, np.nan)\n",
    "        df_out = df_out.where(~df_out.isna(), \"-\")\n",
    "        display(df_out)\n",
    "        df_out.to_csv(f\"output/{output_path}/tables/state_change_{definition}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_latest_common_comparison(df_clean, definitions, other_vars, output_path,mindate=False):\n",
    "    for definition in definitions:\n",
    "        vars = [s for s in other_vars_combined if s.startswith(definition)]\n",
    "        # display(vars)\n",
    "        if mindate:\n",
    "            df_clean = df_clean[df_clean[f\"{definition}_date\"]==\"1900-01-01\"]\n",
    "            display(\"mindate\")\n",
    "        df_subset = df_clean.loc[~df_clean[definition].isna()]\n",
    "        # display(\"remove na\")\n",
    "        # display(df_subset)\n",
    "        # display(vars)\n",
    "        df_subset=df_subset[[definition]+vars].set_index(definition)\n",
    "        # display(\"remove columns\")\n",
    "        # display(df_subset)\n",
    "        df_subset=df_subset.replace(0,np.nan)\n",
    "        # display(\"replace 0s\")\n",
    "        # display(df_subset)\n",
    "        df_subset2 = df_subset.where(df_subset.eq(df_subset.max(1),axis=0))\n",
    "        # display(\"not sure\")\n",
    "        # display(df_subset2)\n",
    "        df_subset_3 = df_subset2.notnull().astype('int').reset_index()\n",
    "        df_sum = redact_round_table(df_subset_3.groupby(definition).sum())\n",
    "        display(df_sum)\n",
    "        # df_sum.to_csv(f'output/{output_path}/tables/simple_latest_common_{definition}.csv')\n",
    "\n",
    "def simple_state_change(df_clean, definitions, other_vars, output_path):\n",
    "    for definition in definitions:\n",
    "        vars = [s for s in other_vars if s.startswith(definition)]\n",
    "        df_subset = (\n",
    "            df_clean[[definition] + vars]\n",
    "            .replace(0, np.nan)\n",
    "            .set_index(definition)\n",
    "            .reset_index()\n",
    "        )\n",
    "        df_subset[\"n\"] = 1\n",
    "        # Count\n",
    "        df_subset2 = df_subset.loc[~df_subset[definition].isna()]\n",
    "        df_subset3 = redact_round_table(\n",
    "            df_subset2.groupby(definition).count()\n",
    "        ).reset_index()\n",
    "        df_subset3.to_csv(f\"output/{output_path}/tables/simple_state_change_{definition}.csv\")\n",
    "\n",
    "def simple_patient_counts(\n",
    "    df_clean,\n",
    "    definitions,\n",
    "    demographic_covariates,\n",
    "    clinical_covariates,\n",
    "    output_path,\n",
    "    categories=False,\n",
    "):\n",
    "    suffix = \"_filled\"\n",
    "    subgroup = \"with records\"\n",
    "    if categories == True:\n",
    "        li_cat_def = []\n",
    "        li_cat = (\n",
    "            df_clean[definitions[0]]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .sort_values()\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        for x in li_cat:\n",
    "            for definition in definitions:\n",
    "                df_clean.loc[df_clean[definition] == x, f\"{x}_{definition}_filled\"] = 1\n",
    "                li_cat_def.append(f\"{x}_{definition}\")\n",
    "        definitions = li_cat_def\n",
    "\n",
    "    # All with measurement\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_temp = (\n",
    "            df_clean[[\"patient_id\", definition + suffix]]\n",
    "            .drop_duplicates()\n",
    "            .dropna()\n",
    "            .set_index(\"patient_id\")\n",
    "        )\n",
    "        li_filled.append(df_temp)\n",
    "\n",
    "    df_temp = (\n",
    "        df_clean[[\"patient_id\", \"all_filled\",\"all_missing\"]]\n",
    "        .drop_duplicates()\n",
    "        .dropna()\n",
    "        .set_index(\"patient_id\")\n",
    "    )\n",
    "    li_filled.append(df_temp)\n",
    "\n",
    "    df_temp2 = pd.concat(li_filled, axis=1)\n",
    "    df_temp2[\"population\"] = 1\n",
    "    # Remove list from memory\n",
    "    del li_filled\n",
    "    df_all = pd.DataFrame(df_temp2.sum()).T\n",
    "    df_all[\"group\"], df_all[\"subgroup\"] = [\"all\", subgroup]\n",
    "    df_all = df_all.set_index([\"group\", \"subgroup\"])\n",
    "\n",
    "    # By group\n",
    "    li_group = []\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        li_filled_group = []\n",
    "        for definition in definitions:\n",
    "            df_temp = (\n",
    "                df_clean[[\"patient_id\", definition + suffix, group]]\n",
    "                .drop_duplicates()\n",
    "                .dropna()\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            li_filled_group.append(df_temp)\n",
    "\n",
    "        df_temp = (\n",
    "            df_clean[[\"patient_id\", \"all_filled\",\"all_missing\", group]]\n",
    "            .drop_duplicates()\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        li_filled_group.append(df_temp)\n",
    "\n",
    "        df_reduce = reduce(\n",
    "            lambda df1, df2: pd.merge(df1, df2, on=[\"patient_id\", group], how=\"outer\"),\n",
    "            li_filled_group,\n",
    "        )\n",
    "        df_reduce[\"population\"] = 1\n",
    "        # Remove list from memory\n",
    "        del li_filled_group\n",
    "        df_reduce2 = (\n",
    "            df_reduce.sort_values(by=group)\n",
    "            .drop(columns=[\"patient_id\"])\n",
    "            .groupby(group)\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        df_reduce2[\"group\"] = group\n",
    "        df_reduce2 = df_reduce2.rename(columns={group: \"subgroup\"})\n",
    "        li_group.append(df_reduce2)\n",
    "    df_all_group = pd.concat(li_group, axis=0, ignore_index=True).set_index(\n",
    "        [\"group\", \"subgroup\"]\n",
    "    )\n",
    "    # Remove list from memory\n",
    "    del li_group\n",
    "\n",
    "    # Redact\n",
    "    df_append = redact_round_table(df_all.append(df_all_group))\n",
    "    if categories:\n",
    "        df_append.to_csv(f\"output/{output_path}/tables/simple_patient_counts_categories.csv\")\n",
    "    else:\n",
    "        df_append.to_csv(f\"output/{output_path}/tables/simple_patient_counts.csv\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsetplot import *\n",
    "\n",
    "df_clean = import_clean(\n",
    "        input_path,\n",
    "        definitions,\n",
    "        other_vars_combined,\n",
    "        demographic_covariates,\n",
    "        clinical_covariates,\n",
    "        reg,\n",
    "        null,\n",
    "        date_min,\n",
    "        date_max,\n",
    "        time_delta,\n",
    "        output_path,\n",
    "        code_dict,\n",
    "        dates,\n",
    "        registered,\n",
    "    )\n",
    "\n",
    "from matplotlib import cm\n",
    "# df_clean_c= df_clean.set_index(df_clean.ethnicity_5_filled == 1)\n",
    "# df_clean_c=df_clean_c.set_index(df_clean.ethnicity_new_5_filled == 1, append=True)\n",
    "\n",
    "# df_clean_c[definitions[1]]= df_clean_c[definitions[1]].fillna(\"Unknown\")\n",
    "# display(df_clean_c)\n",
    "# upset = UpSet(df_clean_c,\n",
    "#               intersection_plot_elements=0)\n",
    "\n",
    "# upset.add_stacked_bars(by=definitions[1], colors=cm.Pastel1,\n",
    "#                        title=\"Count by ethnicity\", elements=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# upset.plot()\n",
    "\n",
    "# # fig = plt.figure(figsize=(10, 3))\n",
    "# # plot(upset, element_size=40)\n",
    "# plt.savefig(\n",
    "#                 f\"output/{output_path}/figures/test.png\"\n",
    "#             )\n",
    "# upset_cat=pd.DataFrame(df_clean[definitions[1]])\n",
    "\n",
    "# for definition in definitions[0:2]:\n",
    "#     for var in other_vars:\n",
    "#         upset_cat[f\"{var}_{definition}\"]=df_clean[definition].str.lower()==var\n",
    "\n",
    "# # upset_cat= upset_cat.set_index(upset_cat[f\"{other_vars[0]}_{definitions[0]}\"] == True)\n",
    "# for definition in definitions[0:2]:\n",
    "#     for var in other_vars:\n",
    "#         display(f\"{var}_{definition}\")\n",
    "#         if (var==other_vars[0] and definition==definitions[0]):\n",
    "#             upset_cat=upset_cat.set_index(upset_cat[f\"{var}_{definition}\"] == True)\n",
    "#         else:    \n",
    "#             upset_cat=upset_cat.set_index(upset_cat[f\"{var}_{definition}\"] == True, append=True)\n",
    "#         upset_cat.drop([f\"{var}_{definition}\"], axis=1, inplace=True)\n",
    "\n",
    "# upset_cat[definitions[1]]= upset_cat[definitions[1]].fillna(\"Unknown\")\n",
    "# upset_cat_plot = UpSet(upset_cat,\n",
    "#               intersection_plot_elements=0)\n",
    "\n",
    "# upset_cat_plot.add_stacked_bars(by=definitions[1], colors=cm.Pastel1,\n",
    "#                        title=\"Count by ethnicity\", elements=10)\n",
    "\n",
    "# upset_cat_plot.plot()\n",
    "\n",
    "\n",
    "def upset(df_clean, output_path, comparator_1, comparator_2):\n",
    "    display(df_clean)\n",
    "    upset_df = df_clean.set_index(~df_clean[comparator_1].isnull())\n",
    "    display(upset_df)\n",
    "    upset_df = upset_df.set_index(~upset_df[comparator_2].isnull(), append=True)\n",
    "\n",
    "    upset_df[comparator_1] = upset_df[comparator_1].fillna(\"Unknown\")\n",
    "    display(upset_df)\n",
    "    upset = UpSet(upset_df, intersection_plot_elements=0)\n",
    "\n",
    "    upset.add_stacked_bars(\n",
    "        by=comparator_1, colors=cm.Pastel1, title=\"Count by ethnicity\", elements=10\n",
    "    )\n",
    "\n",
    "    upset.plot()\n",
    "    plt.savefig(f\"output/{output_path}/figures/upset_{comparator_1}_{comparator_2}.png\")\n",
    "\n",
    "\n",
    "def upset_cat(df_clean, output_path, comparator_1, comparator_2, other_vars):\n",
    "    upset_cat_df = pd.DataFrame(df_clean[comparator_1])\n",
    "    for definition in [comparator_1, comparator_2]:\n",
    "        for var in other_vars:\n",
    "            upset_cat_df[f\"{var}_{definition}\"] = (\n",
    "                df_clean[definition].str.lower() == var\n",
    "            )\n",
    "    ######################################\n",
    "    for definition in [comparator_1, comparator_2]:\n",
    "        for var in other_vars:\n",
    "            if var == other_vars[0] and definition == comparator_1:\n",
    "                upset_cat_df = upset_cat_df.set_index(\n",
    "                    upset_cat_df[f\"{var}_{definition}\"] == True\n",
    "                )\n",
    "            else:\n",
    "                upset_cat_df = upset_cat_df.set_index(\n",
    "                    upset_cat_df[f\"{var}_{definition}\"] == True, append=True\n",
    "                )\n",
    "            display(upset_cat_df)\n",
    "            upset_cat_df.drop([f\"{var}_{definition}\"], axis=1, inplace=True)\n",
    "    ######################################\n",
    "    display(upset_cat_df)\n",
    "    upset_cat_df[comparator_1] = upset_cat_df[comparator_1].fillna(\"Unknown\")\n",
    "    display(upset_cat_df)\n",
    "    upset_cat = UpSet(upset_cat_df, intersection_plot_elements=0)\n",
    "    upset_cat.add_stacked_bars(\n",
    "        by=comparator_1, colors=cm.Pastel1, title=\"Count by ethnicity\", elements=10\n",
    "    )\n",
    "\n",
    "    upset_cat.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>ethnicity_5</th>\n",
       "      <th>ethnicity_new_5</th>\n",
       "      <th>ethnicity_primis_5</th>\n",
       "      <th>ethnicity_5_white</th>\n",
       "      <th>ethnicity_5_mixed</th>\n",
       "      <th>ethnicity_5_asian</th>\n",
       "      <th>ethnicity_5_black</th>\n",
       "      <th>ethnicity_5_other</th>\n",
       "      <th>ethnicity_new_5_white</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_new_5_date</th>\n",
       "      <th>ethnicity_primis_5_date</th>\n",
       "      <th>ethnicity_5_filled</th>\n",
       "      <th>ethnicity_5_missing</th>\n",
       "      <th>ethnicity_new_5_filled</th>\n",
       "      <th>ethnicity_new_5_missing</th>\n",
       "      <th>ethnicity_primis_5_filled</th>\n",
       "      <th>ethnicity_primis_5_missing</th>\n",
       "      <th>all_filled</th>\n",
       "      <th>all_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Asian</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>9812</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>9925</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Other</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>9947</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>9961</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>9970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id ethnicity_5 ethnicity_new_5 ethnicity_primis_5  \\\n",
       "0            61       Asian           Mixed              Asian   \n",
       "1            62       White             NaN                NaN   \n",
       "2            92       Mixed           Other              Asian   \n",
       "3            96       Other             NaN                NaN   \n",
       "4           123         NaN           Mixed              Other   \n",
       "..          ...         ...             ...                ...   \n",
       "645        9812       White           White                NaN   \n",
       "646        9925       Asian           Other              Black   \n",
       "647        9947       Black           Black              White   \n",
       "648        9961       Asian           Mixed              White   \n",
       "649        9970         NaN           White              Black   \n",
       "\n",
       "     ethnicity_5_white  ethnicity_5_mixed  ethnicity_5_asian  \\\n",
       "0                    6                  4                  0   \n",
       "1                    0                  0                  0   \n",
       "2                    0                  0                  0   \n",
       "3                    0                  0                  5   \n",
       "4                    0                  0                  0   \n",
       "..                 ...                ...                ...   \n",
       "645                  0                  0                  0   \n",
       "646                  0                  4                  0   \n",
       "647                  0                  0                  0   \n",
       "648                  0                  0                  0   \n",
       "649                  0                  0                  0   \n",
       "\n",
       "     ethnicity_5_black  ethnicity_5_other  ethnicity_new_5_white  ...  \\\n",
       "0                    0                  6                      0  ...   \n",
       "1                    0                  0                      0  ...   \n",
       "2                    0                  6                      6  ...   \n",
       "3                    0                  0                      0  ...   \n",
       "4                   10                  0                      6  ...   \n",
       "..                 ...                ...                    ...  ...   \n",
       "645                  5                  0                      0  ...   \n",
       "646                  0                  0                      0  ...   \n",
       "647                  9                  0                      0  ...   \n",
       "648                  0                  0                      0  ...   \n",
       "649                  0                  0                      0  ...   \n",
       "\n",
       "     ethnicity_new_5_date  ethnicity_primis_5_date  ethnicity_5_filled  \\\n",
       "0              1900-01-01               1900-01-01                   1   \n",
       "1              1900-01-01               1900-01-01                   1   \n",
       "2              1900-01-01                      NaT                   1   \n",
       "3              1900-01-01                      NaT                   1   \n",
       "4              1900-01-01                      NaT                   0   \n",
       "..                    ...                      ...                 ...   \n",
       "645            1900-01-01                      NaT                   1   \n",
       "646            1900-01-01               1900-01-01                   1   \n",
       "647                   NaT               1900-01-01                   1   \n",
       "648            1900-01-01               1900-01-01                   1   \n",
       "649            1900-01-01                      NaT                   0   \n",
       "\n",
       "     ethnicity_5_missing  ethnicity_new_5_filled  ethnicity_new_5_missing  \\\n",
       "0                      0                       1                        0   \n",
       "1                      0                       0                        1   \n",
       "2                      0                       1                        0   \n",
       "3                      0                       0                        1   \n",
       "4                      1                       1                        0   \n",
       "..                   ...                     ...                      ...   \n",
       "645                    0                       1                        0   \n",
       "646                    0                       1                        0   \n",
       "647                    0                       1                        0   \n",
       "648                    0                       1                        0   \n",
       "649                    1                       1                        0   \n",
       "\n",
       "     ethnicity_primis_5_filled  ethnicity_primis_5_missing  all_filled  \\\n",
       "0                            1                           0           1   \n",
       "1                            0                           1           0   \n",
       "2                            1                           0           1   \n",
       "3                            0                           1           0   \n",
       "4                            1                           0           0   \n",
       "..                         ...                         ...         ...   \n",
       "645                          0                           1           0   \n",
       "646                          1                           0           1   \n",
       "647                          1                           0           1   \n",
       "648                          1                           0           1   \n",
       "649                          1                           0           0   \n",
       "\n",
       "    all_missing  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "..          ...  \n",
       "645           0  \n",
       "646           0  \n",
       "647           0  \n",
       "648           0  \n",
       "649           0  \n",
       "\n",
       "[650 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>ethnicity_5</th>\n",
       "      <th>ethnicity_new_5</th>\n",
       "      <th>ethnicity_primis_5</th>\n",
       "      <th>ethnicity_5_white</th>\n",
       "      <th>ethnicity_5_mixed</th>\n",
       "      <th>ethnicity_5_asian</th>\n",
       "      <th>ethnicity_5_black</th>\n",
       "      <th>ethnicity_5_other</th>\n",
       "      <th>ethnicity_new_5_white</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_new_5_date</th>\n",
       "      <th>ethnicity_primis_5_date</th>\n",
       "      <th>ethnicity_5_filled</th>\n",
       "      <th>ethnicity_5_missing</th>\n",
       "      <th>ethnicity_new_5_filled</th>\n",
       "      <th>ethnicity_new_5_missing</th>\n",
       "      <th>ethnicity_primis_5_filled</th>\n",
       "      <th>ethnicity_primis_5_missing</th>\n",
       "      <th>all_filled</th>\n",
       "      <th>all_missing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity_5</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>61</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Asian</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>62</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>92</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>96</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9812</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9925</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Other</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9947</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9961</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>9970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             patient_id ethnicity_5 ethnicity_new_5 ethnicity_primis_5  \\\n",
       "ethnicity_5                                                              \n",
       "True                 61       Asian           Mixed              Asian   \n",
       "True                 62       White             NaN                NaN   \n",
       "True                 92       Mixed           Other              Asian   \n",
       "True                 96       Other             NaN                NaN   \n",
       "False               123         NaN           Mixed              Other   \n",
       "...                 ...         ...             ...                ...   \n",
       "True               9812       White           White                NaN   \n",
       "True               9925       Asian           Other              Black   \n",
       "True               9947       Black           Black              White   \n",
       "True               9961       Asian           Mixed              White   \n",
       "False              9970         NaN           White              Black   \n",
       "\n",
       "             ethnicity_5_white  ethnicity_5_mixed  ethnicity_5_asian  \\\n",
       "ethnicity_5                                                            \n",
       "True                         6                  4                  0   \n",
       "True                         0                  0                  0   \n",
       "True                         0                  0                  0   \n",
       "True                         0                  0                  5   \n",
       "False                        0                  0                  0   \n",
       "...                        ...                ...                ...   \n",
       "True                         0                  0                  0   \n",
       "True                         0                  4                  0   \n",
       "True                         0                  0                  0   \n",
       "True                         0                  0                  0   \n",
       "False                        0                  0                  0   \n",
       "\n",
       "             ethnicity_5_black  ethnicity_5_other  ethnicity_new_5_white  ...  \\\n",
       "ethnicity_5                                                               ...   \n",
       "True                         0                  6                      0  ...   \n",
       "True                         0                  0                      0  ...   \n",
       "True                         0                  6                      6  ...   \n",
       "True                         0                  0                      0  ...   \n",
       "False                       10                  0                      6  ...   \n",
       "...                        ...                ...                    ...  ...   \n",
       "True                         5                  0                      0  ...   \n",
       "True                         0                  0                      0  ...   \n",
       "True                         9                  0                      0  ...   \n",
       "True                         0                  0                      0  ...   \n",
       "False                        0                  0                      0  ...   \n",
       "\n",
       "             ethnicity_new_5_date  ethnicity_primis_5_date  \\\n",
       "ethnicity_5                                                  \n",
       "True                   1900-01-01               1900-01-01   \n",
       "True                   1900-01-01               1900-01-01   \n",
       "True                   1900-01-01                      NaT   \n",
       "True                   1900-01-01                      NaT   \n",
       "False                  1900-01-01                      NaT   \n",
       "...                           ...                      ...   \n",
       "True                   1900-01-01                      NaT   \n",
       "True                   1900-01-01               1900-01-01   \n",
       "True                          NaT               1900-01-01   \n",
       "True                   1900-01-01               1900-01-01   \n",
       "False                  1900-01-01                      NaT   \n",
       "\n",
       "             ethnicity_5_filled  ethnicity_5_missing  ethnicity_new_5_filled  \\\n",
       "ethnicity_5                                                                    \n",
       "True                          1                    0                       1   \n",
       "True                          1                    0                       0   \n",
       "True                          1                    0                       1   \n",
       "True                          1                    0                       0   \n",
       "False                         0                    1                       1   \n",
       "...                         ...                  ...                     ...   \n",
       "True                          1                    0                       1   \n",
       "True                          1                    0                       1   \n",
       "True                          1                    0                       1   \n",
       "True                          1                    0                       1   \n",
       "False                         0                    1                       1   \n",
       "\n",
       "             ethnicity_new_5_missing  ethnicity_primis_5_filled  \\\n",
       "ethnicity_5                                                       \n",
       "True                               0                          1   \n",
       "True                               1                          0   \n",
       "True                               0                          1   \n",
       "True                               1                          0   \n",
       "False                              0                          1   \n",
       "...                              ...                        ...   \n",
       "True                               0                          0   \n",
       "True                               0                          1   \n",
       "True                               0                          1   \n",
       "True                               0                          1   \n",
       "False                              0                          1   \n",
       "\n",
       "             ethnicity_primis_5_missing  all_filled all_missing  \n",
       "ethnicity_5                                                      \n",
       "True                                  0           1           0  \n",
       "True                                  1           0           0  \n",
       "True                                  0           1           0  \n",
       "True                                  1           0           0  \n",
       "False                                 0           0           0  \n",
       "...                                 ...         ...         ...  \n",
       "True                                  1           0           0  \n",
       "True                                  0           1           0  \n",
       "True                                  0           1           0  \n",
       "True                                  0           1           0  \n",
       "False                                 0           0           0  \n",
       "\n",
       "[650 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>ethnicity_5</th>\n",
       "      <th>ethnicity_new_5</th>\n",
       "      <th>ethnicity_primis_5</th>\n",
       "      <th>ethnicity_5_white</th>\n",
       "      <th>ethnicity_5_mixed</th>\n",
       "      <th>ethnicity_5_asian</th>\n",
       "      <th>ethnicity_5_black</th>\n",
       "      <th>ethnicity_5_other</th>\n",
       "      <th>ethnicity_new_5_white</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_new_5_date</th>\n",
       "      <th>ethnicity_primis_5_date</th>\n",
       "      <th>ethnicity_5_filled</th>\n",
       "      <th>ethnicity_5_missing</th>\n",
       "      <th>ethnicity_new_5_filled</th>\n",
       "      <th>ethnicity_new_5_missing</th>\n",
       "      <th>ethnicity_primis_5_filled</th>\n",
       "      <th>ethnicity_primis_5_missing</th>\n",
       "      <th>all_filled</th>\n",
       "      <th>all_missing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity_5</th>\n",
       "      <th>ethnicity_new_5</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th>True</th>\n",
       "      <td>61</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Asian</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>62</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>92</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>96</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>123</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th>True</th>\n",
       "      <td>9812</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9925</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Other</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9947</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>9961</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>9970</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>White</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             patient_id ethnicity_5 ethnicity_new_5  \\\n",
       "ethnicity_5 ethnicity_new_5                                           \n",
       "True        True                     61       Asian           Mixed   \n",
       "            False                    62       White             NaN   \n",
       "            True                     92       Mixed           Other   \n",
       "            False                    96       Other             NaN   \n",
       "False       True                    123     Unknown           Mixed   \n",
       "...                                 ...         ...             ...   \n",
       "True        True                   9812       White           White   \n",
       "            True                   9925       Asian           Other   \n",
       "            True                   9947       Black           Black   \n",
       "            True                   9961       Asian           Mixed   \n",
       "False       True                   9970     Unknown           White   \n",
       "\n",
       "                            ethnicity_primis_5  ethnicity_5_white  \\\n",
       "ethnicity_5 ethnicity_new_5                                         \n",
       "True        True                         Asian                  6   \n",
       "            False                          NaN                  0   \n",
       "            True                         Asian                  0   \n",
       "            False                          NaN                  0   \n",
       "False       True                         Other                  0   \n",
       "...                                        ...                ...   \n",
       "True        True                           NaN                  0   \n",
       "            True                         Black                  0   \n",
       "            True                         White                  0   \n",
       "            True                         White                  0   \n",
       "False       True                         Black                  0   \n",
       "\n",
       "                             ethnicity_5_mixed  ethnicity_5_asian  \\\n",
       "ethnicity_5 ethnicity_new_5                                         \n",
       "True        True                             4                  0   \n",
       "            False                            0                  0   \n",
       "            True                             0                  0   \n",
       "            False                            0                  5   \n",
       "False       True                             0                  0   \n",
       "...                                        ...                ...   \n",
       "True        True                             0                  0   \n",
       "            True                             4                  0   \n",
       "            True                             0                  0   \n",
       "            True                             0                  0   \n",
       "False       True                             0                  0   \n",
       "\n",
       "                             ethnicity_5_black  ethnicity_5_other  \\\n",
       "ethnicity_5 ethnicity_new_5                                         \n",
       "True        True                             0                  6   \n",
       "            False                            0                  0   \n",
       "            True                             0                  6   \n",
       "            False                            0                  0   \n",
       "False       True                            10                  0   \n",
       "...                                        ...                ...   \n",
       "True        True                             5                  0   \n",
       "            True                             0                  0   \n",
       "            True                             9                  0   \n",
       "            True                             0                  0   \n",
       "False       True                             0                  0   \n",
       "\n",
       "                             ethnicity_new_5_white  ...  ethnicity_new_5_date  \\\n",
       "ethnicity_5 ethnicity_new_5                         ...                         \n",
       "True        True                                 0  ...            1900-01-01   \n",
       "            False                                0  ...            1900-01-01   \n",
       "            True                                 6  ...            1900-01-01   \n",
       "            False                                0  ...            1900-01-01   \n",
       "False       True                                 6  ...            1900-01-01   \n",
       "...                                            ...  ...                   ...   \n",
       "True        True                                 0  ...            1900-01-01   \n",
       "            True                                 0  ...            1900-01-01   \n",
       "            True                                 0  ...                   NaT   \n",
       "            True                                 0  ...            1900-01-01   \n",
       "False       True                                 0  ...            1900-01-01   \n",
       "\n",
       "                             ethnicity_primis_5_date  ethnicity_5_filled  \\\n",
       "ethnicity_5 ethnicity_new_5                                                \n",
       "True        True                          1900-01-01                   1   \n",
       "            False                         1900-01-01                   1   \n",
       "            True                                 NaT                   1   \n",
       "            False                                NaT                   1   \n",
       "False       True                                 NaT                   0   \n",
       "...                                              ...                 ...   \n",
       "True        True                                 NaT                   1   \n",
       "            True                          1900-01-01                   1   \n",
       "            True                          1900-01-01                   1   \n",
       "            True                          1900-01-01                   1   \n",
       "False       True                                 NaT                   0   \n",
       "\n",
       "                             ethnicity_5_missing  ethnicity_new_5_filled  \\\n",
       "ethnicity_5 ethnicity_new_5                                                \n",
       "True        True                               0                       1   \n",
       "            False                              0                       0   \n",
       "            True                               0                       1   \n",
       "            False                              0                       0   \n",
       "False       True                               1                       1   \n",
       "...                                          ...                     ...   \n",
       "True        True                               0                       1   \n",
       "            True                               0                       1   \n",
       "            True                               0                       1   \n",
       "            True                               0                       1   \n",
       "False       True                               1                       1   \n",
       "\n",
       "                             ethnicity_new_5_missing  \\\n",
       "ethnicity_5 ethnicity_new_5                            \n",
       "True        True                                   0   \n",
       "            False                                  1   \n",
       "            True                                   0   \n",
       "            False                                  1   \n",
       "False       True                                   0   \n",
       "...                                              ...   \n",
       "True        True                                   0   \n",
       "            True                                   0   \n",
       "            True                                   0   \n",
       "            True                                   0   \n",
       "False       True                                   0   \n",
       "\n",
       "                             ethnicity_primis_5_filled  \\\n",
       "ethnicity_5 ethnicity_new_5                              \n",
       "True        True                                     1   \n",
       "            False                                    0   \n",
       "            True                                     1   \n",
       "            False                                    0   \n",
       "False       True                                     1   \n",
       "...                                                ...   \n",
       "True        True                                     0   \n",
       "            True                                     1   \n",
       "            True                                     1   \n",
       "            True                                     1   \n",
       "False       True                                     1   \n",
       "\n",
       "                             ethnicity_primis_5_missing  all_filled  \\\n",
       "ethnicity_5 ethnicity_new_5                                           \n",
       "True        True                                      0           1   \n",
       "            False                                     1           0   \n",
       "            True                                      0           1   \n",
       "            False                                     1           0   \n",
       "False       True                                      0           0   \n",
       "...                                                 ...         ...   \n",
       "True        True                                      1           0   \n",
       "            True                                      0           1   \n",
       "            True                                      0           1   \n",
       "            True                                      0           1   \n",
       "False       True                                      0           0   \n",
       "\n",
       "                            all_missing  \n",
       "ethnicity_5 ethnicity_new_5              \n",
       "True        True                      0  \n",
       "            False                     0  \n",
       "            True                      0  \n",
       "            False                     0  \n",
       "False       True                      0  \n",
       "...                                 ...  \n",
       "True        True                      0  \n",
       "            True                      0  \n",
       "            True                      0  \n",
       "            True                      0  \n",
       "False       True                      0  \n",
       "\n",
       "[650 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x384 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "upset(df_clean, output_path, definitions[0],definitions[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd7deed66f04c8f1cb07fbe5b34625baefe8e7d41af3b53c9662fc8ba397df1b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
