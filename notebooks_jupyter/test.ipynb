{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import file\n",
    "input_path = 'C:/Users/candrews/Documents/GitHub/ethnicity-short-data-report/output/data/input.feather'\n",
    "\n",
    "# Definition\n",
    "definitions = ['ethnicity_5', 'ethnicity_new_5', 'ethnicity_primis_5']\n",
    "\n",
    "# Code dictionary\n",
    "code_dict = {\n",
    "    'imd': {0: 'Unknown', 1: '1 Most deprived', 2: '2', 3: '3', 4: '4', 5: '5 Least deprived'},\n",
    "    'ethnicity_5': {1:'White', 2:'Mixed', 3:'Asian', 4:'Black', 5:'Other'},\n",
    "    'ethnicity_new_5': {1:'White', 2:'Mixed', 3:'Asian', 4:'Black', 5:'Other'},\n",
    "    'ethnicity_primis_5': {1:'White', 2:'Mixed', 3:'Asian', 4:'Black', 5:'Other'},\n",
    "}\n",
    "\n",
    "# Other variables to include\n",
    "other_vars = ['mixed','white','asian','black','other']\n",
    "other_vars_combined = [x+'_'+y for x in definitions for y in other_vars]\n",
    "\n",
    "registered=True\n",
    "reg='registered'\n",
    "\n",
    "# Dates\n",
    "dates = False\n",
    "date_min = ''\n",
    "date_max = ''\n",
    "time_delta = ''\n",
    "\n",
    "# Min/max range\n",
    "min_range = 4\n",
    "max_range = 200\n",
    "\n",
    "# Null value – could be multiple values in a list [0,'0',NA]\n",
    "null = [0,\"0\"]\n",
    "\n",
    "# Covariates\n",
    "demographic_covariates = ['age_band', 'sex', 'region', 'imd']\n",
    "clinical_covariates = ['dementia', 'diabetes', 'hypertension', 'learning_disability']\n",
    "\n",
    "# Output filepath\n",
    "output_path = 'phenotype_validation_ethnicity/5'\n",
    "if registered:\n",
    "    output_path = output_path + \"/registered\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#from ebmdatalab import charts\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def redact_round_table(df_in):\n",
    "    \"\"\"Redacts counts <= 5 and rounds counts to nearest 5\"\"\"\n",
    "    df_out = df_in.where(df_in > 5, np.nan).apply(lambda x: 5 * round(x/5))\n",
    "    return df_out\n",
    "\n",
    "def import_clean(input_path, definitions, other_vars, demographic_covariates, \n",
    "                 clinical_covariates, null, date_min, date_max, \n",
    "                 time_delta, output_path, code_dict='', dates=False,registered=True):\n",
    "    # Import\n",
    "    df_import = pd.read_feather(input_path)\n",
    "    # restrict to registered as of index date\n",
    "    if registered==True:\n",
    "        df_import=df_import[df_import[reg]]\n",
    "    # Dates\n",
    "    if dates==True:\n",
    "        date_vars = [definition+'_date' for definition in definitions]\n",
    "        # Create variable that captures difference in measurement dates\n",
    "        date_diff_vars = []\n",
    "        # Define start and end dates\n",
    "        start_date = datetime.datetime.strptime(date_min, '%Y-%m-%d')\n",
    "        end_date = datetime.datetime.strptime(date_max, '%Y-%m-%d')\n",
    "        for definition in definitions:\n",
    "            # Remove OpenSAFELY null dates \n",
    "            df_import.loc[df_import[definition+'_date'] == '1900-01-01', definition+'_date'] = np.nan\n",
    "            # Limit to period of interest             \n",
    "            df_import[definition+'_date'] = pd.to_datetime(df_import[definition+'_date'])\n",
    "            df_import.loc[df_import[definition+'_date'] < start_date, definition+'_date'] = np.nan\n",
    "            df_import.loc[df_import[definition+'_date'] > end_date, definition+'_date'] = np.nan\n",
    "            # Remove the measurement if outside the date parameters\n",
    "            df_import.loc[df_import[definition+'_date'].isna(), definition] = np.nan\n",
    "            df_import \n",
    "            # Create difference between measurement dates\n",
    "            df_import[definition+'_date'] = df_import[definition+'_date'].dt.to_period(time_delta).dt.to_timestamp()\n",
    "            df_import = df_import.sort_values(by=['patient_id',definition+'_date'])\n",
    "            df_import['date_diff_' + definition] = round(df_import.groupby('patient_id')[definition+'_date'].diff() / np.timedelta64(1, time_delta))\n",
    "            date_diff_vars.append('date_diff_' + definition)\n",
    "    else: \n",
    "        date_vars = []\n",
    "        date_diff_vars = []\n",
    "    # Codes\n",
    "    if code_dict!='':\n",
    "        for key in code_dict:\n",
    "            df_import[key] = df_import[key].astype(float)\n",
    "            df_import[key] = df_import[key].replace(code_dict[key])\n",
    "    # Subset to relevant columns\n",
    "    df_clean = df_import[['patient_id'] + definitions + other_vars + date_vars + date_diff_vars + demographic_covariates + clinical_covariates]\n",
    "    # Limit to relevant date range\n",
    "    df_clean = df_clean.sort_values(by='patient_id').reset_index(drop=True)\n",
    "    # Set null values to nan\n",
    "    for definition in definitions: \n",
    "        df_clean.loc[df_clean[definition].isin(null), definition] = np.nan\n",
    "     # Create order for categorical variables\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        if df_clean[group].dtype.name == 'category':\n",
    "            li_order = sorted(df_clean[group].dropna().unique().tolist())\n",
    "            df_clean[group] = df_clean[group].cat.reorder_categories(li_order, ordered=True)\n",
    "    # Mark patients with value filled/missing for each definition\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_fill = pd.DataFrame(df_clean.groupby(\"patient_id\")[definition].any().astype('int')).rename(\n",
    "            columns={definition:definition+'_filled'}\n",
    "        )\n",
    "        df_fill[definition+'_missing'] = 1-df_fill[definition+'_filled']\n",
    "        li_filled.append(df_fill)\n",
    "\n",
    "    df_filled = pd.concat(li_filled, axis=1)\n",
    "    # Remove list from memory\n",
    "    del li_filled  \n",
    "    df_clean = df_clean.merge(df_filled, on='patient_id')\n",
    "    \n",
    "    # Flag all filled/all missing\n",
    "    li_col_filled = [col for col in df_clean.columns if col.endswith('_filled')]\n",
    "    li_col_missing = [col for col in df_clean.columns if col.endswith('_missing')]\n",
    "    df_clean['all_filled'] = (df_clean[li_col_filled].sum(axis=1) == len(definitions)).astype(int)\n",
    "    df_clean['all_missing'] = (df_clean[li_col_missing].sum(axis=1) == len(definitions)).astype(int)\n",
    "    \n",
    "    # Check whether output paths exist or not, create if missing\n",
    "    path_tables = f'output/{output_path}/tables'\n",
    "    path_figures = f'output/{output_path}/figures'\n",
    "    li_filepaths = [path_tables, path_figures]\n",
    "\n",
    "    for filepath in li_filepaths:\n",
    "        exists = os.path.exists(filepath)\n",
    "        if not exists:\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def patient_counts(df_clean, definitions, demographic_covariates, clinical_covariates, output_path,code_dict='',categories=False, missing=False):\n",
    "    suffix = '_filled'\n",
    "    subgroup = 'with records'\n",
    "    overlap = 'all_filled'\n",
    "    if missing == True:\n",
    "        suffix = '_missing'\n",
    "        subgroup = 'missing records'\n",
    "        overlap = 'all_missing'\n",
    "    if categories == True:\n",
    "        li_cat_def = []\n",
    "        li_cat = df_clean[definitions[0]].dropna().astype(str).sort_values().unique().tolist()\n",
    "        for x in li_cat:\n",
    "            for definition in definitions:\n",
    "                df_clean.loc[df_clean[definition] == x, f'{x}_{definition}_filled'] = 1 \n",
    "                li_cat_def.append(f'-{x}-{definition}')\n",
    "        if code_dict!='':\n",
    "            for i in code_dict[definition]:\n",
    "                li_cat_def = list(map(lambda x: x.replace(code_dict[definition][i],str(i) ), li_cat_def))\n",
    "            \n",
    "            li_cat_def=sorted(li_cat_def)\n",
    "            for i in code_dict[definition]:\n",
    "                li_cat_def = list(map(lambda x: x.replace(f\"-{str(i)}-\",f\"{code_dict[definition][i]}_\"), li_cat_def))\n",
    "        definitions = li_cat_def\n",
    "        display(definitions)\n",
    "    \n",
    "    # All with measurement\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_temp = df_clean[['patient_id', definition+suffix]].drop_duplicates().dropna().set_index('patient_id')\n",
    "        li_filled.append(df_temp)\n",
    "\n",
    "    df_temp = df_clean[['patient_id', overlap]].drop_duplicates().dropna().set_index('patient_id')\n",
    "    li_filled.append(df_temp)\n",
    "    \n",
    "    df_temp2 = pd.concat(li_filled, axis=1)\n",
    "    df_temp2['population'] = 1\n",
    "    # Remove list from memory\n",
    "    del li_filled\n",
    "    df_all = pd.DataFrame(df_temp2.sum()).T\n",
    "    df_all['group'],df_all['subgroup'] = ['all',subgroup]\n",
    "    df_all = df_all.set_index(['group','subgroup'])\n",
    "    \n",
    "    # By group\n",
    "    li_group = []\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        li_filled_group = []\n",
    "        for definition in definitions:\n",
    "            df_temp = df_clean[['patient_id', definition+suffix, group]].drop_duplicates().dropna().reset_index(drop=True)\n",
    "            li_filled_group.append(df_temp)\n",
    "            \n",
    "        df_temp = df_clean[['patient_id', overlap, group]].drop_duplicates().dropna().reset_index(drop=True)\n",
    "        li_filled_group.append(df_temp)\n",
    "        \n",
    "        df_reduce = reduce(lambda df1, df2: pd.merge(df1, df2,on=['patient_id',group],how='outer'), li_filled_group)\n",
    "        df_reduce['population'] = 1\n",
    "        # Remove list from memory\n",
    "        del li_filled_group \n",
    "        df_reduce2 = df_reduce.sort_values(by=group).drop(columns=['patient_id']).groupby(group).sum().reset_index()\n",
    "        df_reduce2['group'] = group\n",
    "        df_reduce2 = df_reduce2.rename(columns={group:'subgroup'})\n",
    "        li_group.append(df_reduce2)\n",
    "    df_all_group = pd.concat(li_group, axis=0, ignore_index=True).set_index(['group','subgroup'])\n",
    "    # Remove list from memory\n",
    "    del li_group \n",
    "    \n",
    "    # Redact\n",
    "    df_append = redact_round_table(df_all.append(df_all_group))\n",
    "        \n",
    "    # Create percentage columns \n",
    "    for definition in definitions:\n",
    "        df_append[definition+'_pct'] = round((df_append[definition+suffix].div(df_append['population']))*100,1)\n",
    "    df_append[overlap+'_pct'] = round((df_append[overlap].div(df_append['population']))*100,1)\n",
    "\n",
    "    # Final redaction step\n",
    "    df_append = df_append.where(~df_append.isna(), '-')  \n",
    "\n",
    "    # Combine count and percentage columns\n",
    "    for definition in definitions:\n",
    "        df_append[definition] = df_append[definition+suffix].astype(str) + \" (\" + df_append[definition+'_pct'].astype(str) + \")\" \n",
    "        df_append = df_append.drop(columns=[definition+suffix,definition+'_pct'])\n",
    "    df_append[overlap] = df_append[overlap].astype(str) + \" (\" + df_append[overlap+'_pct'].astype(str) + \")\" \n",
    "    df_append = df_append.drop(columns=[overlap+'_pct'])\n",
    "    \n",
    "    # Column order\n",
    "    li_col_order = []\n",
    "    for definition in definitions:\n",
    "        li_col_order.append(definition)\n",
    "    li_col_order.append(overlap)\n",
    "    li_col_order.append('population')\n",
    "\n",
    "    df_all_redact = df_append[li_col_order]\n",
    "    df_all_redact.columns=df_all_redact.columns.str.replace('_', ' ')\n",
    "    display(df_all_redact)\n",
    "    if categories == False:\n",
    "        df_all_redact.to_csv(f'output/{output_path}/tables/patient_counts{suffix}.csv')\n",
    "    if categories == True:\n",
    "        df_all_redact.to_csv(f'output/{output_path}/tables/patient_counts_by_categories{suffix}.csv')\n",
    "\n",
    "def display_heatmap(df_clean, definitions, output_path):\n",
    "    # All with measurement\n",
    "    li_filled = []\n",
    "    for definition in definitions:\n",
    "        df_temp = df_clean[['patient_id']].drop_duplicates().set_index('patient_id')\n",
    "        df_temp[definition+'_filled'] = 1\n",
    "        df_temp = df_clean[['patient_id', definition+'_filled']].drop_duplicates().dropna().set_index('patient_id')\n",
    "        li_filled.append(df_temp)\n",
    "\n",
    "    # Prepare data for heatmap input\n",
    "    df_temp2 = pd.concat(li_filled, axis=1)\n",
    "    # Remove list from memory\n",
    "    del li_filled \n",
    "    df_transform = df_temp2.replace(np.nan,0)\n",
    "    df_dot = redact_round_table(df_transform.T.dot(df_transform))\n",
    "    \n",
    "    # Create mask to eliminate duplicates in heatmap\n",
    "    mask = np.triu(np.ones_like(df_dot))\n",
    "    np.fill_diagonal(mask[::1], 0)\n",
    "\n",
    "    # Draw the heatmap with the mask\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.heatmap(df_dot, annot=True, mask=mask, fmt='g', cmap=\"YlGnBu\", vmin=0)\n",
    "    #plt.show()\n",
    "    plt.savefig(f'output/{output_path}/figures/heatmap.png')\n",
    "\n",
    "def records_over_time(df_clean, definitions, demographic_covariates, clinical_covariates, output_path):\n",
    "    li_df = []\n",
    "    for definition in definitions:\n",
    "        df_grouped = df_clean[[definition+'_date',definition]].groupby(definition+'_date').count().reset_index().rename(columns={definition+'_date':'date'}).set_index('date')\n",
    "        li_df.append(redact_round_table(df_grouped))\n",
    "    df_all_time = pd.concat(li_df).stack().reset_index().rename(columns={'level_1':'variable',0:'value'})\n",
    "    # Remove list from memory\n",
    "    del li_df \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    fig.autofmt_xdate()\n",
    "    sns.lineplot(x = 'date', y = 'value', hue='variable', data = df_all_time, ax=ax).set_title('New records by month')\n",
    "    ax.legend().set_title('')\n",
    "    plt.savefig(f'output/{output_path}/figures/records_over_time.png')\n",
    "\n",
    "    for group in demographic_covariates + clinical_covariates:\n",
    "        for definition in definitions:\n",
    "            df_grouped = df_clean[[definition+'_date',definition,group]].groupby(\n",
    "                                  [definition+'_date',group]).count().reset_index().rename(columns={definition+'_date':'date'}).set_index(['date', group])\n",
    "            df_time=redact_round_table(df_grouped).reset_index()\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            fig.autofmt_xdate()\n",
    "            sns.lineplot(x = 'date', y = definition, hue=group, data = df_time, ax=ax).set_title(f'{definition} recorded by {group} and month')\n",
    "            ax.legend().set_title('')\n",
    "            plt.savefig(f'output/{output_path}/figures/records_over_time_{definition}_{group}.png')\n",
    "            \n",
    "def report_distribution(df_occ, definitions, num_definitions, output_path, group=''):\n",
    "    \"\"\"\n",
    "    Plots histogram or boxplots of distribution\n",
    "    \"\"\"\n",
    "    if group == '':\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions: \n",
    "\n",
    "                avg_value = pd.DataFrame(\n",
    "                    df_occ[definition].agg(\n",
    "                        ['mean','count']\n",
    "                    )\n",
    "                )\n",
    "                if avg_value.loc['count'][0] > 6:\n",
    "                    avg_value.loc['count'][0] = 5 * round(avg_value.loc['count'][0]/5)\n",
    "                    print(f'Average {definition}:\\n')\n",
    "                    #display(avg_value)\n",
    "                    avg_value.to_csv(f'output/{output_path}/tables/avg_value_{definition}.csv')\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    hist_data = df_occ[definition].loc[~df_occ[definition].isna()]\n",
    "                    plt.hist(hist_data, bins=np.arange(min(hist_data), max(hist_data)))\n",
    "                    plt.title('Distribution of ' + definition)\n",
    "                    #plt.show()\n",
    "                    plt.savefig(f'output/{output_path}/figures/distribution.png')\n",
    "                else:\n",
    "                    print('Table and plot redacted due to low counts.')\n",
    "                    \n",
    "        else:\n",
    "            df_bp = df_occ[definitions]\n",
    "            avg = pd.DataFrame(df_bp.mean(),columns=['mean'])\n",
    "            ct = pd.DataFrame(df_bp.count(),columns=['count'])\n",
    "            avg_value = avg.merge(ct, left_index=True, right_index=True)\n",
    "            # Redact and round values\n",
    "            avg_value['count'] = avg_value['count'].where(\n",
    "                avg_value['count'] > 5, np.nan).apply(lambda x: 5 * round(x/5) if ~np.isnan(x) else x)\n",
    "            print('Averages:\\n')\n",
    "            #display(avg_value)\n",
    "            avg_value.to_csv(f'output/{output_path}/tables/avg_value.csv')\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            sns.boxplot(data=df_bp,showfliers = False)\n",
    "            plt.title(\"Distributions of values\")\n",
    "            #plt.show()\n",
    "            plt.savefig(f'output/{output_path}/figures/distribution.png')\n",
    "    else:\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions: \n",
    "                df_bp = df_occ[[group]+ [definition]]\n",
    "                avg_value = df_bp.groupby(group)[definition].agg(\n",
    "                    ['mean', 'count']\n",
    "                )\n",
    "                # Redact and round values\n",
    "                avg_value['count'] = avg_value['count'].where(\n",
    "                    avg_value['count'] > 5, np.nan).apply(lambda x: 5 * round(x/5) if ~np.isnan(x) else x)\n",
    "                avg_value.loc[avg_value['count'].isna(), ['count','mean']] = ['-','-']\n",
    "                print(f'Averages by {group}:\\n')\n",
    "                #display(avg_value)    \n",
    "                avg_value.to_csv(f'output/{output_path}/tables/avg_value_{definition}_{group}.csv')\n",
    "                null_index = avg_value[avg_value['count'] == '-'].index.tolist()\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(x=group, y=definition, data=df_bp.loc[~df_bp[group].isin(null_index)], showfliers=False)\n",
    "                plt.title(f\"Distributions by {group}\")\n",
    "                #plt.show()\n",
    "                plt.savefig(f'output/{output_path}/figures/distribution_{group}.png')\n",
    "        else:\n",
    "            if df_occ[group].dtype == 'bool':\n",
    "                df_occ[group] = df_occ[group].apply(lambda x: str(x))\n",
    "            df_occ = df_occ.loc[~df_occ[group].isna()] # Drop nan categories\n",
    "            df_bp = df_occ[[group] + definitions]\n",
    "            avg = df_bp.groupby(group).mean().add_prefix(\"avg_\")\n",
    "            ct = df_bp.groupby(group).count().add_prefix(\"ct_\")\n",
    "            avg_value = avg.merge(ct, left_on=group, right_on=group)\n",
    "            for definition in definitions:\n",
    "                # Redact and round values\n",
    "                avg_value['ct_'+definition] = avg_value['ct_'+definition].where(\n",
    "                    avg_value['ct_'+definition] > 5, np.nan).apply(lambda x: 5 * round(x/5) if ~np.isnan(x) else x)\n",
    "                avg_value.loc[avg_value['ct_'+definition].isna(), \n",
    "                                    ['ct_'+definition,'avg_'+definition]] = ['-','-']\n",
    "            print(f'Averages by {group}:\\n')\n",
    "            #display(avg_value)\n",
    "            avg_value.to_csv(f'output/{output_path}/tables/avg_value_{group}.csv')\n",
    "            for definition in definitions:\n",
    "                null_index = []\n",
    "                null_index = avg_value[avg_value['ct_'+definition] == '-'].index.tolist()\n",
    "                df_bp.loc[df_bp[group].isin(null_index),definition] = np.nan\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            df_plot = df_bp.melt(id_vars=group, value_vars=definitions)\n",
    "            sns.boxplot(x=group, y='value', hue='variable', data=df_plot, showfliers=False)\n",
    "            plt.title(f'Distributions by {group}')\n",
    "            #plt.show()\n",
    "            plt.savefig(f'output/{output_path}/figures/distribution_{group}.png')\n",
    "            \n",
    "def report_out_of_range(df_occ, definitions, min_range, max_range, num_definitions, null, output_path, group=''):\n",
    "    \"\"\"\n",
    "    Reports number of measurements outside of defined range\n",
    "    \"\"\"\n",
    "    \n",
    "    def q25(x):\n",
    "        return x.quantile(0.25)\n",
    "    def q75(x):\n",
    "        return x.quantile(0.75)\n",
    "    \n",
    "    li_dfs = []\n",
    "    \n",
    "    df_oor = df_occ\n",
    "    for definition in definitions: \n",
    "        df_oor.loc[(df_oor[definition] < min_range) | (df_oor[definition] > max_range), \"out_of_range_\"+definition] = 1\n",
    "        # Make definitions null if not out of range or empty\n",
    "        df_oor[\"oor_\" + definition] = df_oor[definition]\n",
    "        df_oor.loc[(df_oor[\"out_of_range_\"+definition] != 1) | (df_oor[definition].isin(null)), \"oor_\" + definition] = np.nan\n",
    "        if group == '':\n",
    "            try:\n",
    "                df_out = df_oor.agg(\n",
    "                                    count = (\"oor_\" + definition, 'count'),\n",
    "                                    mean  = (\"oor_\" + definition, 'mean'),\n",
    "                                    pct25 = (\"oor_\" + definition,q25),\n",
    "                                    pct75 = (\"oor_\" + definition,q75),\n",
    "                                    )\n",
    "            except:\n",
    "                df_out = pd.DataFrame([['count', 0],['mean',np.nan],\n",
    "                                       ['pct25',np.nan],['pct75',np.nan]], \n",
    "                                      columns=['index',\"oor_\" + definition]).set_index('index')\n",
    "            if df_out.loc['count'][\"oor_\" + definition] > 6:\n",
    "                df_out.loc['count'][\"oor_\" + definition] = 5 * round(df_out.loc['count'][\"oor_\" + definition]/5)\n",
    "            else:\n",
    "                df_out[\"oor_\" + definition] = '-'\n",
    "        else:\n",
    "            df_out = df_oor.groupby(group)[\"oor_\" + definition].agg(\n",
    "                                                [('count', 'count'),\n",
    "                                                 ('mean', 'mean'),\n",
    "                                                 ('pct25', q25),\n",
    "                                                 ('pct75', q75)]\n",
    "                                              ).add_suffix(\"_\"+definition)\n",
    "            df_out.loc[df_out[\"count_\" + definition] > 5, \"count_\" + definition] = 5 * round(df_out[\"count_\" + definition]/5)\n",
    "            df_out.loc[df_out[\"count_\" + definition] < 6, \n",
    "                       [\"count_\" + definition, \"mean_\" + definition,\n",
    "                       \"pct25_\" + definition, \"pct75_\" + definition]] = ['-','-','-','-']\n",
    "        li_dfs.append(df_out)    \n",
    "    \n",
    "    if num_definitions == 1:    \n",
    "        #display(df_out)\n",
    "        df_out.to_csv(f'output/{output_path}/tables/out_of_range.csv')\n",
    "        # Remove list from memory\n",
    "        del li_dfs \n",
    "        if group == '': \n",
    "            if df_out[\"oor_\" + definition]['count'] != '-':\n",
    "                df_plot = df_oor[\"oor_\" + definition]\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                plt.hist(df_plot)\n",
    "                plt.title('Distribution of out of range ' + definition)\n",
    "                #plt.show()\n",
    "                plt.savefig(f'output/{output_path}/figures/out_of_range.png')\n",
    "            else:\n",
    "                print('Plot redacted due to low counts.')\n",
    "        else:\n",
    "            df_oor = df_oor.loc[~df_oor[group].isna()]\n",
    "            for definition in definitions: \n",
    "                null_index = df_out[df_out['count_'+definition] == '-'].index.tolist()\n",
    "                df_oor.loc[df_oor[group].isin(null_index),'oor_'+definition] = np.nan\n",
    "                df_bp = df_oor[[group]+ [\"oor_\" + definition]]\n",
    "                if df_bp[\"oor_\" + definition].sum() > 0:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    sns.boxplot(x=group, y=\"oor_\" + definition, data=df_bp, showfliers=False)\n",
    "                    plt.title(f\"Distribution of out of range values by {group}\")\n",
    "                    plt.show()\n",
    "                    plt.savefig(f'output/{output_path}/figures/out_of_range_{group}.png')\n",
    "                else:\n",
    "                    print('Plot redacted due to low counts.')\n",
    "    else:\n",
    "        df_merged = reduce(lambda left,right: pd.merge(left,right,left_index=True, right_index=True), li_dfs)\n",
    "        # Remove list from memory\n",
    "        del li_dfs \n",
    "        #display(df_merged)\n",
    "        if group == '':    \n",
    "            df_merged.to_csv(f'output/{output_path}/tables/out_of_range.csv')\n",
    "            cols = [\"oor_\" + definition for definition in definitions]\n",
    "            df_bp = df_oor[cols]\n",
    "            if df_merged[\"oor_\" + definition]['count'] == '-':\n",
    "                df_bp[\"oor_\" + definition] = np.nan\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(data=df_bp, showfliers=False)\n",
    "                plt.title('Distribution of out of range values')\n",
    "                #plt.show()\n",
    "                plt.savefig(f'output/{output_path}/figures/out_of_range.png')\n",
    "            except: \n",
    "                print('Plot redacted due to low counts.')\n",
    "        else:\n",
    "            df_merged.to_csv(f'output/{output_path}/tables/out_of_range_{group}.csv')\n",
    "            df_oor = df_oor.loc[~df_oor[group].isna()]\n",
    "            for definition in definitions: \n",
    "                null_index = df_merged[df_merged['count_'+definition] == '-'].index.tolist()\n",
    "                df_oor.loc[df_oor[group].isin(null_index),'oor_'+definition] = np.nan\n",
    "            if df_oor[group].dtype == 'bool':\n",
    "                df_oor[group] = df_oor[group].apply(lambda x: str(x))\n",
    "            cols = [\"oor_\" + definition for definition in definitions]\n",
    "            df_bp = df_oor[[group] + cols]\n",
    "            df_plot = df_bp.melt(id_vars=group, value_vars=cols)\n",
    "            if df_plot['value'].sum() > 0:\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(x=group, y='value', hue='variable', data=df_plot, showfliers=False)\n",
    "                plt.title(f'Distribution of out of range values by {group}')\n",
    "                #plt.show()\n",
    "                plt.savefig(f'output/{output_path}/figures/out_of_range_{group}.png')\n",
    "            else: \n",
    "                print('Plot redacted due to low counts.')\n",
    "        \n",
    "def report_update_frequency(df_occ, definitions, time_delta, num_definitions, output_path, group=''):\n",
    "    \"\"\"\n",
    "    Plots histogram or boxplot of update frequency and reports average update frequency\n",
    "    \"\"\"\n",
    "    if group == '':\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions: \n",
    "                avg_update_freq = df_occ.agg(\n",
    "                    avg_diff = (f'date_diff_{definition}', 'mean'),\n",
    "                    count = (f'date_diff_{definition}' , 'count')\n",
    "                )\n",
    "                if avg_update_freq.loc['count'][0] > 6:\n",
    "                    avg_update_freq.loc['count'][0] = 5 * round(avg_update_freq.loc['count'][0]/5)\n",
    "                    print(f'Average update frequency of {definition} by {time_delta}:\\n')\n",
    "                    #display(avg_update_freq)\n",
    "                    avg_update_freq.to_csv(f'output/{output_path}/tables/avg_update_frequency_{definition}.csv')\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    plt.hist(df_occ['date_diff_' + definition])\n",
    "                    plt.title('Update frequency of ' + definition + f\" by {time_delta}\")\n",
    "                    #plt.show()\n",
    "                    plt.savefig(f'output/{output_path}/figures/avg_update_frequency_{definition}.png')\n",
    "                else:\n",
    "                    print('Table and plot redacted due to low counts.')\n",
    "        else:\n",
    "            cols = ['date_diff_' + x for x in definitions]\n",
    "            df_bp = df_occ[cols]\n",
    "            avg_update = pd.DataFrame(df_bp.mean(),columns=['avg_diff'])\n",
    "            ct_update = pd.DataFrame(df_bp.count(),columns=['count'])\n",
    "            avg_update_freq = avg_update.merge(ct_update, left_index=True, right_index=True)\n",
    "            # Redact and round values\n",
    "            avg_update_freq['count'] = avg_update_freq['count'].where(\n",
    "                avg_update_freq['count'] > 5, np.nan).apply(lambda x: 5 * round(x/5) if ~np.isnan(x) else x)\n",
    "            print(f'Average update frequency by {time_delta}:\\n')\n",
    "            #display(avg_update_freq)    \n",
    "            avg_update_freq.to_csv(f'output/{output_path}/tables/avg_update_frequency.csv')\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            null_index = avg_update_freq[avg_update_freq['count'] == '-'].index.tolist()\n",
    "            sns.boxplot(data=df_bp.drop(columns=null_index), showfliers=False)\n",
    "            plt.title(f\"Update frequency by {time_delta}\")\n",
    "            #plt.show()\n",
    "            plt.savefig(f'output/{output_path}/figures/avg_update_frequency.png')\n",
    "          \n",
    "    else:\n",
    "        if num_definitions == 1:\n",
    "            for definition in definitions: \n",
    "                df_bp = df_occ[[group]+ ['date_diff_' + definition]]\n",
    "                avg_update_freq = df_occ.groupby(group).agg(\n",
    "                    avg_diff = ('date_diff_' + definition, 'mean'),\n",
    "                    count = ('date_diff_' + definition, 'count')\n",
    "                ).reset_index()\n",
    "                # Redact and round values\n",
    "                avg_update_freq['count'] = avg_update_freq['count'].where(\n",
    "                    avg_update_freq['count'] > 5, np.nan).apply(lambda x: 5 * round(x/5) if ~np.isnan(x) else x)\n",
    "                avg_update_freq.loc[avg_update_freq['count'].isna(), ['count','avg_diff']] = ['-','-']\n",
    "                print(f'Average update frequency by {group} and {time_delta}:\\n')\n",
    "                #display(avg_update_freq)    \n",
    "                avg_update_freq.to_csv(f'output/{output_path}/tables/avg_update_frequency_{definition}.csv')\n",
    "                null_index = avg_update_freq[avg_update_freq['count'] == '-'].index.tolist()\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                sns.boxplot(x=group, y='date_diff_'+definition, data=df_bp.loc[~df_bp[group].isin(null_index)].sort_index(), showfliers=False)\n",
    "                plt.title(f\"Update frequency by {group} and {time_delta}\")\n",
    "                #plt.show()\n",
    "                plt.savefig(f'output/{output_path}/figures/avg_update_frequency_{definition}.png')\n",
    "        else:\n",
    "            if df_occ[group].dtype == 'bool':\n",
    "                df_occ[group] = df_occ[group].apply(lambda x: str(x))\n",
    "            df_occ = df_occ.loc[~df_occ[group].isna()] # Drop nan categories\n",
    "            cols = ['date_diff_' + x for x in definitions]\n",
    "            df_sub = df_occ[[group] + cols]\n",
    "            avg_update = df_sub.groupby(group).mean().add_prefix(\"avg_\")\n",
    "            ct_update = df_sub.groupby(group).count().add_prefix(\"ct_\")\n",
    "            avg_update_freq = avg_update.merge(ct_update, left_on=group, right_on=group).sort_index()\n",
    "            for definition in definitions:\n",
    "                # Redact and round values\n",
    "                avg_update_freq['ct_date_diff_'+definition] = avg_update_freq['ct_date_diff_'+definition].where(\n",
    "                    avg_update_freq['ct_date_diff_'+definition] > 5, np.nan).apply(lambda x: 5 * round(x/5) if ~np.isnan(x) else x)\n",
    "                avg_update_freq.loc[avg_update_freq['ct_date_diff_'+definition].isna(), \n",
    "                                    ['ct_date_diff_'+definition,'avg_date_diff_'+definition]] = ['-','-']\n",
    "            # Sort by index\n",
    "            print(f'Average update frequencies by {time_delta}:\\n')\n",
    "            #display(avg_update_freq)\n",
    "            avg_update_freq.to_csv(f'output/{output_path}/tables/avg_update_frequency_{group}.csv')\n",
    "            for definition in definitions:\n",
    "                null_index = []\n",
    "                null_index = avg_update_freq[avg_update_freq['ct_date_diff_'+definition] == '-'].index.tolist()\n",
    "                df_sub.loc[df_sub[group].isin(null_index),'date_diff_'+definition] = np.nan\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            df_plot = df_sub.melt(id_vars=group, value_vars=cols)\n",
    "            sns.boxplot(x=group, y='value', hue='variable', data=df_plot, showfliers=False)\n",
    "            plt.title(f'Update frequencies by {group} and {time_delta}')\n",
    "            #plt.show()\n",
    "            plt.savefig(f'output/{output_path}/figures/avg_update_frequency_{group}.png')\n",
    "            \n",
    "def latest_common_comparison(df_clean, definitions, other_vars, output_path,code_dict=''):\n",
    "    for definition in definitions:\n",
    "        vars = [s for s in other_vars if s.startswith(definition)]\n",
    "        df_subset = df_clean.loc[~df_clean[definition].isna()]\n",
    "        df_subset=df_subset[[definition]+vars].set_index(definition)\n",
    "        df_subset=df_subset.replace(0,np.nan)\n",
    "        df_subset2 = df_subset.where(df_subset.eq(df_subset.max(1),axis=0))\n",
    "        #add check for tied most common ethnicity\n",
    "        # check=df_subset2.count(axis=1)\n",
    "        # check = df_subset2.loc[check>1]\n",
    "        # display(check) \n",
    "        df_subset_3 = df_subset2.notnull().astype('int').reset_index()\n",
    "        df_sum = redact_round_table(df_subset_3.groupby(definition).sum())\n",
    "        df_counts = pd.DataFrame(np.diagonal(df_sum),index=df_sum.index,columns=[f'matching (n={np.diagonal(df_sum).sum()})'])\n",
    "\n",
    "        df_sum2 = df_sum.copy(deep=True)\n",
    "        np.fill_diagonal(df_sum2.values, 0)\n",
    "        df_diag = pd.DataFrame(df_sum2.sum(axis=1), columns=[f'not_matching (n={df_sum2.sum(axis=1).sum()})'])\n",
    "        df_out = df_counts.merge(df_diag,right_index=True,left_index=True)\n",
    "        \n",
    "        #sort rows by category index\n",
    "        df_out=df_out.reindex(list(code_dict[definition].values()))\n",
    "\n",
    "        df_out = df_out.where(~df_out.isna(), '-')\n",
    "        df_out.to_csv(f'output/{output_path}/tables/latest_common_simple_{definition}.csv')\n",
    "        df_sum = redact_round_table(df_subset_3.groupby(definition).sum())   \n",
    "        #sort columns by category index\n",
    "        df_sum.columns=df_sum.columns.str.replace(definition+'_', '')\n",
    "        if code_dict!='':\n",
    "            lowerlist=[x.lower() for x in (list(code_dict[definition].values()))]\n",
    "            df_sum = df_sum[lowerlist]\n",
    "        else:\n",
    "            df_sum = df_sum.reindex(sorted(df_sum.columns), axis=1)       \n",
    "        #sort rows by category index\n",
    "        df_sum=df_sum.reindex(list(code_dict[definition].values()))\n",
    "        df_sum.columns=df_sum.columns.str.replace('_', ' ')\n",
    "        for col in df_sum.columns:\n",
    "            df_sum = df_sum.rename(columns = {col:f'{col} (n={df_sum[col].sum()})'})\n",
    "        df_sum = df_sum.where(~df_sum.isna(), '-')       \n",
    "        df_sum.to_csv(f'output/{output_path}/tables/latest_common_expanded_{definition}.csv')\n",
    "            \n",
    "def state_change(df_clean, definitions, other_vars, output_path,code_dict=''):\n",
    "    for definition in definitions:\n",
    "        vars = [s for s in other_vars if s.startswith(definition)]\n",
    "        df_subset = df_clean[\n",
    "            [definition]+vars\n",
    "        ].replace(0,np.nan).set_index(definition).reset_index()\n",
    "        df_subset['n'] = 1\n",
    "        # Count\n",
    "        df_subset2 = df_subset.loc[~df_subset[definition].isna()]\n",
    "        df_subset3 = redact_round_table(df_subset2.groupby(definition).count()).reset_index()\n",
    "        # Set index\n",
    "        ### arrange rows by category value\n",
    "        df_subset3=df_subset3.set_index(definition)\n",
    "        df_subset3=df_subset3.reindex(list(code_dict[definition].values()))\n",
    "        df_subset3=df_subset3.reset_index()\n",
    "\n",
    "        \n",
    "        df_subset3['index'] = df_subset3[definition].astype(str) + \" (n = \" + df_subset3['n'].astype(int).astype(str) + \")\"\n",
    "        df_out = df_subset3.drop(columns=[definition,'n']).rename(columns = {'index':definition}).set_index(definition)\n",
    "        df_out.columns=df_out.columns.str.replace(definition+'_', '')\n",
    "        #rearrange columns by category value \n",
    "        df_out.columns=df_out.columns.str.lower()\n",
    "        if code_dict!='':\n",
    "            lowerlist=[x.lower() for x in (list(code_dict[definition].values()))]\n",
    "            df_out = df_out[lowerlist]\n",
    "        else:\n",
    "            df_out = df_out.reindex(sorted(df_out.columns), axis=1)        \n",
    "        # Null out the diagonal\n",
    "        # np.fill_diagonal(df_out.values, np.nan)\n",
    "        df_out = df_out.where(~df_out.isna(), '-')\n",
    "        df_out.to_csv(f'output/{output_path}/tables/state_change_{definition}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>died_date_ons</th>\n",
       "      <th>ethnicity_5_date</th>\n",
       "      <th>ethnicity_16_date</th>\n",
       "      <th>ethnicity_new_5_date</th>\n",
       "      <th>ethnicity_new_16_date</th>\n",
       "      <th>ethnicity_primis_5_date</th>\n",
       "      <th>ethnicity_primis_16_date</th>\n",
       "      <th>sev_obesity</th>\n",
       "      <th>age</th>\n",
       "      <th>age_band</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_primis_16_Indian</th>\n",
       "      <th>ethnicity_primis_16_Pakistani</th>\n",
       "      <th>ethnicity_primis_16_Bangladeshi</th>\n",
       "      <th>ethnicity_primis_16_Other_Asian</th>\n",
       "      <th>ethnicity_primis_16_Caribbean</th>\n",
       "      <th>ethnicity_primis_16_African</th>\n",
       "      <th>ethnicity_primis_16_Other_Black</th>\n",
       "      <th>ethnicity_primis_16_Chinese</th>\n",
       "      <th>ethnicity_primis_16_Any_other_ethnic_group</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>20</td>\n",
       "      <td>50-59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1935-01-01</td>\n",
       "      <td>1965-01-01</td>\n",
       "      <td>1946-01-01</td>\n",
       "      <td>1965-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2</td>\n",
       "      <td>70-79</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1928-01-01</td>\n",
       "      <td>1947-01-01</td>\n",
       "      <td>1906-01-01</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>47</td>\n",
       "      <td>80+</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1916-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>54</td>\n",
       "      <td>30-39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1910-01-01</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>29</td>\n",
       "      <td>0-19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>NaT</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1909-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>1925-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>46</td>\n",
       "      <td>30-39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>1942-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1922-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>69</td>\n",
       "      <td>30-39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1969-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1942-01-01</td>\n",
       "      <td>1928-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1914-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>85</td>\n",
       "      <td>20-29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1906-01-01</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1989-01-01</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>12</td>\n",
       "      <td>40-49</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1956-01-01</td>\n",
       "      <td>1958-01-01</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1907-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>24</td>\n",
       "      <td>20-29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    died_date_ons ethnicity_5_date ethnicity_16_date ethnicity_new_5_date  \\\n",
       "0             NaT              NaT               NaT           2014-01-01   \n",
       "1             NaT       1935-01-01        1965-01-01           1946-01-01   \n",
       "2             NaT       1928-01-01        1947-01-01           1906-01-01   \n",
       "3             NaT       1985-01-01        1991-01-01                  NaT   \n",
       "4             NaT              NaT        2020-01-01           1979-01-01   \n",
       "..            ...              ...               ...                  ...   \n",
       "995           NaT       2001-01-01        1909-01-01                  NaT   \n",
       "996           NaT              NaT        1988-01-01           1942-01-01   \n",
       "997           NaT       1969-01-01               NaT           1942-01-01   \n",
       "998           NaT       1906-01-01        1961-01-01                  NaT   \n",
       "999           NaT       1956-01-01        1958-01-01           1975-01-01   \n",
       "\n",
       "    ethnicity_new_16_date ethnicity_primis_5_date ethnicity_primis_16_date  \\\n",
       "0              2011-01-01              1977-01-01               1988-01-01   \n",
       "1              1965-01-01                     NaT                      NaT   \n",
       "2              2010-01-01                     NaT                      NaT   \n",
       "3                     NaT                     NaT               1916-01-01   \n",
       "4                     NaT              1910-01-01               2021-01-01   \n",
       "..                    ...                     ...                      ...   \n",
       "995            2009-01-01              1966-01-01               1925-01-01   \n",
       "996                   NaT                     NaT               1922-01-01   \n",
       "997            1928-01-01                     NaT               1914-01-01   \n",
       "998            1989-01-01              1994-01-01                      NaT   \n",
       "999                   NaT              1907-01-01                      NaT   \n",
       "\n",
       "    sev_obesity  age age_band  ... ethnicity_primis_16_Indian  \\\n",
       "0           NaT   20    50-59  ...                          0   \n",
       "1           NaT    2    70-79  ...                          0   \n",
       "2           NaT   47      80+  ...                          0   \n",
       "3           NaT   54    30-39  ...                          0   \n",
       "4           NaT   29     0-19  ...                          0   \n",
       "..          ...  ...      ...  ...                        ...   \n",
       "995         NaT   46    30-39  ...                          0   \n",
       "996         NaT   69    30-39  ...                          0   \n",
       "997         NaT   85    20-29  ...                          0   \n",
       "998         NaT   12    40-49  ...                          0   \n",
       "999         NaT   24    20-29  ...                          0   \n",
       "\n",
       "     ethnicity_primis_16_Pakistani ethnicity_primis_16_Bangladeshi  \\\n",
       "0                                0                               0   \n",
       "1                                0                               0   \n",
       "2                                0                               0   \n",
       "3                                0                               0   \n",
       "4                                0                               0   \n",
       "..                             ...                             ...   \n",
       "995                              0                               0   \n",
       "996                              0                               0   \n",
       "997                              0                               0   \n",
       "998                              0                               0   \n",
       "999                              0                               0   \n",
       "\n",
       "    ethnicity_primis_16_Other_Asian  ethnicity_primis_16_Caribbean  \\\n",
       "0                                 0                              0   \n",
       "1                                 1                              0   \n",
       "2                                 0                              0   \n",
       "3                                 0                              0   \n",
       "4                                 0                              0   \n",
       "..                              ...                            ...   \n",
       "995                               0                              0   \n",
       "996                               0                              0   \n",
       "997                               0                              0   \n",
       "998                               0                              0   \n",
       "999                               0                              0   \n",
       "\n",
       "    ethnicity_primis_16_African ethnicity_primis_16_Other_Black  \\\n",
       "0                             0                               0   \n",
       "1                             0                               7   \n",
       "2                             0                               0   \n",
       "3                             1                               0   \n",
       "4                             0                               0   \n",
       "..                          ...                             ...   \n",
       "995                           0                               0   \n",
       "996                           0                               0   \n",
       "997                           0                               0   \n",
       "998                           0                               0   \n",
       "999                           0                               0   \n",
       "\n",
       "    ethnicity_primis_16_Chinese ethnicity_primis_16_Any_other_ethnic_group  \\\n",
       "0                             0                                          0   \n",
       "1                             9                                          0   \n",
       "2                             0                                          0   \n",
       "3                             0                                          0   \n",
       "4                             0                                          0   \n",
       "..                          ...                                        ...   \n",
       "995                           0                                          9   \n",
       "996                           0                                          0   \n",
       "997                           0                                          0   \n",
       "998                           0                                          0   \n",
       "999                           0                                          0   \n",
       "\n",
       "    patient_id  \n",
       "0         3203  \n",
       "1         5701  \n",
       "2         9643  \n",
       "3         3690  \n",
       "4         5988  \n",
       "..         ...  \n",
       "995       9603  \n",
       "996       3867  \n",
       "997       5795  \n",
       "998       5281  \n",
       "999       3213  \n",
       "\n",
       "[1000 rows x 97 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>died_date_ons</th>\n",
       "      <th>ethnicity_5_date</th>\n",
       "      <th>ethnicity_16_date</th>\n",
       "      <th>ethnicity_new_5_date</th>\n",
       "      <th>ethnicity_new_16_date</th>\n",
       "      <th>ethnicity_primis_5_date</th>\n",
       "      <th>ethnicity_primis_16_date</th>\n",
       "      <th>sev_obesity</th>\n",
       "      <th>age</th>\n",
       "      <th>age_band</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_primis_16_Indian</th>\n",
       "      <th>ethnicity_primis_16_Pakistani</th>\n",
       "      <th>ethnicity_primis_16_Bangladeshi</th>\n",
       "      <th>ethnicity_primis_16_Other_Asian</th>\n",
       "      <th>ethnicity_primis_16_Caribbean</th>\n",
       "      <th>ethnicity_primis_16_African</th>\n",
       "      <th>ethnicity_primis_16_Other_Black</th>\n",
       "      <th>ethnicity_primis_16_Chinese</th>\n",
       "      <th>ethnicity_primis_16_Any_other_ethnic_group</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1935-01-01</td>\n",
       "      <td>1965-01-01</td>\n",
       "      <td>1946-01-01</td>\n",
       "      <td>1965-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2</td>\n",
       "      <td>70-79</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1928-01-01</td>\n",
       "      <td>1947-01-01</td>\n",
       "      <td>1906-01-01</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>47</td>\n",
       "      <td>80+</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1916-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>54</td>\n",
       "      <td>30-39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1907-01-01</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1948-01-01</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>38</td>\n",
       "      <td>60-69</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1922-01-01</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>1942-01-01</td>\n",
       "      <td>1904-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>14</td>\n",
       "      <td>80+</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1901-01-01</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>1930-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>87</td>\n",
       "      <td>80+</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1908-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1911-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>40</td>\n",
       "      <td>70-79</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1920-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>35</td>\n",
       "      <td>50-59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>1942-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1922-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>69</td>\n",
       "      <td>30-39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NaT</td>\n",
       "      <td>1969-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1942-01-01</td>\n",
       "      <td>1928-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1914-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>85</td>\n",
       "      <td>20-29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    died_date_ons ethnicity_5_date ethnicity_16_date ethnicity_new_5_date  \\\n",
       "1             NaT       1935-01-01        1965-01-01           1946-01-01   \n",
       "2             NaT       1928-01-01        1947-01-01           1906-01-01   \n",
       "3             NaT       1985-01-01        1991-01-01                  NaT   \n",
       "8             NaT       1907-01-01        1966-01-01                  NaT   \n",
       "10            NaT       1922-01-01        1966-01-01                  NaT   \n",
       "..            ...              ...               ...                  ...   \n",
       "991           NaT       1901-01-01        1985-01-01           1930-01-01   \n",
       "993           NaT       1908-01-01               NaT           1911-01-01   \n",
       "994           NaT       1920-01-01               NaT                  NaT   \n",
       "996           NaT              NaT        1988-01-01           1942-01-01   \n",
       "997           NaT       1969-01-01               NaT           1942-01-01   \n",
       "\n",
       "    ethnicity_new_16_date ethnicity_primis_5_date ethnicity_primis_16_date  \\\n",
       "1              1965-01-01                     NaT                      NaT   \n",
       "2              2010-01-01                     NaT                      NaT   \n",
       "3                     NaT                     NaT               1916-01-01   \n",
       "8                     NaT              1948-01-01               1997-01-01   \n",
       "10             2000-01-01              1942-01-01               1904-01-01   \n",
       "..                    ...                     ...                      ...   \n",
       "991                   NaT                     NaT                      NaT   \n",
       "993                   NaT              1975-01-01                      NaT   \n",
       "994                   NaT                     NaT               1998-01-01   \n",
       "996                   NaT                     NaT               1922-01-01   \n",
       "997            1928-01-01                     NaT               1914-01-01   \n",
       "\n",
       "    sev_obesity  age age_band  ... ethnicity_primis_16_Indian  \\\n",
       "1           NaT    2    70-79  ...                          0   \n",
       "2           NaT   47      80+  ...                          0   \n",
       "3           NaT   54    30-39  ...                          0   \n",
       "8           NaT   38    60-69  ...                          0   \n",
       "10          NaT   14      80+  ...                          0   \n",
       "..          ...  ...      ...  ...                        ...   \n",
       "991         NaT   87      80+  ...                          0   \n",
       "993         NaT   40    70-79  ...                          0   \n",
       "994         NaT   35    50-59  ...                          0   \n",
       "996         NaT   69    30-39  ...                          0   \n",
       "997         NaT   85    20-29  ...                          0   \n",
       "\n",
       "     ethnicity_primis_16_Pakistani ethnicity_primis_16_Bangladeshi  \\\n",
       "1                                0                               0   \n",
       "2                                0                               0   \n",
       "3                                0                               0   \n",
       "8                                0                               0   \n",
       "10                               0                               0   \n",
       "..                             ...                             ...   \n",
       "991                              0                               0   \n",
       "993                              0                               0   \n",
       "994                              0                               0   \n",
       "996                              0                               0   \n",
       "997                              0                               0   \n",
       "\n",
       "    ethnicity_primis_16_Other_Asian  ethnicity_primis_16_Caribbean  \\\n",
       "1                                 1                              0   \n",
       "2                                 0                              0   \n",
       "3                                 0                              0   \n",
       "8                                 9                              0   \n",
       "10                                0                              0   \n",
       "..                              ...                            ...   \n",
       "991                               0                              0   \n",
       "993                               0                              0   \n",
       "994                               0                              3   \n",
       "996                               0                              0   \n",
       "997                               0                              0   \n",
       "\n",
       "    ethnicity_primis_16_African ethnicity_primis_16_Other_Black  \\\n",
       "1                             0                               7   \n",
       "2                             0                               0   \n",
       "3                             1                               0   \n",
       "8                             0                               0   \n",
       "10                            0                               0   \n",
       "..                          ...                             ...   \n",
       "991                           0                               0   \n",
       "993                           0                               0   \n",
       "994                           0                               0   \n",
       "996                           0                               0   \n",
       "997                           0                               0   \n",
       "\n",
       "    ethnicity_primis_16_Chinese ethnicity_primis_16_Any_other_ethnic_group  \\\n",
       "1                             9                                          0   \n",
       "2                             0                                          0   \n",
       "3                             0                                          0   \n",
       "8                             0                                          0   \n",
       "10                            0                                          0   \n",
       "..                          ...                                        ...   \n",
       "991                           0                                          0   \n",
       "993                           9                                          0   \n",
       "994                           9                                          0   \n",
       "996                           0                                          0   \n",
       "997                           0                                          0   \n",
       "\n",
       "    patient_id  \n",
       "1         5701  \n",
       "2         9643  \n",
       "3         3690  \n",
       "8         4115  \n",
       "10        9509  \n",
       "..         ...  \n",
       "991        595  \n",
       "993       2727  \n",
       "994       5022  \n",
       "996       3867  \n",
       "997       5795  \n",
       "\n",
       "[650 rows x 97 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(output_path)\n",
    "# output_path2=output_path+\"/this\"\n",
    "# display(output_path2)\n",
    "df_import = pd.read_feather(input_path)\n",
    "display(df_import)\n",
    "df_import = df_import[df_import[reg]]\n",
    "display(df_import)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ethnicity 5</th>\n",
       "      <th>ethnicity new 5</th>\n",
       "      <th>ethnicity primis 5</th>\n",
       "      <th>all filled</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th>subgroup</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <th>with records</th>\n",
       "      <td>480.0 (73.8)</td>\n",
       "      <td>455.0 (70.0)</td>\n",
       "      <td>490.0 (75.4)</td>\n",
       "      <td>265.0 (40.8)</td>\n",
       "      <td>650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">age_band</th>\n",
       "      <th>0-19</th>\n",
       "      <td>65.0 (72.2)</td>\n",
       "      <td>60.0 (66.7)</td>\n",
       "      <td>65.0 (72.2)</td>\n",
       "      <td>35.0 (38.9)</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-29</th>\n",
       "      <td>65.0 (76.5)</td>\n",
       "      <td>65.0 (76.5)</td>\n",
       "      <td>65.0 (76.5)</td>\n",
       "      <td>35.0 (41.2)</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-39</th>\n",
       "      <td>50.0 (66.7)</td>\n",
       "      <td>45.0 (60.0)</td>\n",
       "      <td>55.0 (73.3)</td>\n",
       "      <td>25.0 (33.3)</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-49</th>\n",
       "      <td>60.0 (75.0)</td>\n",
       "      <td>60.0 (75.0)</td>\n",
       "      <td>55.0 (68.8)</td>\n",
       "      <td>35.0 (43.8)</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-59</th>\n",
       "      <td>50.0 (66.7)</td>\n",
       "      <td>50.0 (66.7)</td>\n",
       "      <td>55.0 (73.3)</td>\n",
       "      <td>25.0 (33.3)</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-69</th>\n",
       "      <td>70.0 (77.8)</td>\n",
       "      <td>65.0 (72.2)</td>\n",
       "      <td>75.0 (83.3)</td>\n",
       "      <td>45.0 (50.0)</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70-79</th>\n",
       "      <td>65.0 (72.2)</td>\n",
       "      <td>55.0 (61.1)</td>\n",
       "      <td>65.0 (72.2)</td>\n",
       "      <td>30.0 (33.3)</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80+</th>\n",
       "      <td>55.0 (78.6)</td>\n",
       "      <td>50.0 (71.4)</td>\n",
       "      <td>55.0 (78.6)</td>\n",
       "      <td>35.0 (50.0)</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sex</th>\n",
       "      <th>F</th>\n",
       "      <td>225.0 (73.8)</td>\n",
       "      <td>215.0 (70.5)</td>\n",
       "      <td>225.0 (73.8)</td>\n",
       "      <td>115.0 (37.7)</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>250.0 (72.5)</td>\n",
       "      <td>240.0 (69.6)</td>\n",
       "      <td>260.0 (75.4)</td>\n",
       "      <td>150.0 (43.5)</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">region</th>\n",
       "      <th>East Midlands</th>\n",
       "      <td>40.0 (72.7)</td>\n",
       "      <td>40.0 (72.7)</td>\n",
       "      <td>40.0 (72.7)</td>\n",
       "      <td>25.0 (45.5)</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>East of England</th>\n",
       "      <td>25.0 (71.4)</td>\n",
       "      <td>20.0 (57.1)</td>\n",
       "      <td>30.0 (85.7)</td>\n",
       "      <td>15.0 (42.9)</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>London</th>\n",
       "      <td>55.0 (73.3)</td>\n",
       "      <td>55.0 (73.3)</td>\n",
       "      <td>50.0 (66.7)</td>\n",
       "      <td>30.0 (40.0)</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North East</th>\n",
       "      <td>35.0 (77.8)</td>\n",
       "      <td>30.0 (66.7)</td>\n",
       "      <td>40.0 (88.9)</td>\n",
       "      <td>25.0 (55.6)</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North West</th>\n",
       "      <td>25.0 (83.3)</td>\n",
       "      <td>15.0 (50.0)</td>\n",
       "      <td>25.0 (83.3)</td>\n",
       "      <td>10.0 (33.3)</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South East</th>\n",
       "      <td>70.0 (77.8)</td>\n",
       "      <td>70.0 (77.8)</td>\n",
       "      <td>70.0 (77.8)</td>\n",
       "      <td>45.0 (50.0)</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Midlands</th>\n",
       "      <td>35.0 (70.0)</td>\n",
       "      <td>35.0 (70.0)</td>\n",
       "      <td>30.0 (60.0)</td>\n",
       "      <td>20.0 (40.0)</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yorkshire and the Humber</th>\n",
       "      <td>30.0 (85.7)</td>\n",
       "      <td>25.0 (71.4)</td>\n",
       "      <td>25.0 (71.4)</td>\n",
       "      <td>15.0 (42.9)</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">imd</th>\n",
       "      <th>1 Most deprived</th>\n",
       "      <td>110.0 (75.9)</td>\n",
       "      <td>95.0 (65.5)</td>\n",
       "      <td>105.0 (72.4)</td>\n",
       "      <td>60.0 (41.4)</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.0 (69.2)</td>\n",
       "      <td>85.0 (65.4)</td>\n",
       "      <td>100.0 (76.9)</td>\n",
       "      <td>50.0 (38.5)</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0 (76.9)</td>\n",
       "      <td>90.0 (69.2)</td>\n",
       "      <td>90.0 (69.2)</td>\n",
       "      <td>45.0 (34.6)</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.0 (72.0)</td>\n",
       "      <td>95.0 (76.0)</td>\n",
       "      <td>100.0 (80.0)</td>\n",
       "      <td>50.0 (40.0)</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 Least deprived</th>\n",
       "      <td>95.0 (79.2)</td>\n",
       "      <td>85.0 (70.8)</td>\n",
       "      <td>95.0 (79.2)</td>\n",
       "      <td>55.0 (45.8)</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dementia</th>\n",
       "      <th>False</th>\n",
       "      <td>475.0 (73.6)</td>\n",
       "      <td>450.0 (69.8)</td>\n",
       "      <td>480.0 (74.4)</td>\n",
       "      <td>260.0 (40.3)</td>\n",
       "      <td>645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>5.0 (100.0)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>5.0 (100.0)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">diabetes</th>\n",
       "      <th>False</th>\n",
       "      <td>475.0 (74.2)</td>\n",
       "      <td>450.0 (70.3)</td>\n",
       "      <td>480.0 (75.0)</td>\n",
       "      <td>260.0 (40.6)</td>\n",
       "      <td>640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>5.0 (50.0)</td>\n",
       "      <td>10.0 (100.0)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hypertension</th>\n",
       "      <th>False</th>\n",
       "      <td>475.0 (73.6)</td>\n",
       "      <td>450.0 (69.8)</td>\n",
       "      <td>480.0 (74.4)</td>\n",
       "      <td>260.0 (40.3)</td>\n",
       "      <td>645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>5.0 (100.0)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">learning_disability</th>\n",
       "      <th>False</th>\n",
       "      <td>475.0 (74.2)</td>\n",
       "      <td>450.0 (70.3)</td>\n",
       "      <td>480.0 (75.0)</td>\n",
       "      <td>260.0 (40.6)</td>\n",
       "      <td>640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>5.0 (50.0)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>10.0 (100.0)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ethnicity 5 ethnicity new 5  \\\n",
       "group               subgroup                                                 \n",
       "all                 with records              480.0 (73.8)    455.0 (70.0)   \n",
       "age_band            0-19                       65.0 (72.2)     60.0 (66.7)   \n",
       "                    20-29                      65.0 (76.5)     65.0 (76.5)   \n",
       "                    30-39                      50.0 (66.7)     45.0 (60.0)   \n",
       "                    40-49                      60.0 (75.0)     60.0 (75.0)   \n",
       "                    50-59                      50.0 (66.7)     50.0 (66.7)   \n",
       "                    60-69                      70.0 (77.8)     65.0 (72.2)   \n",
       "                    70-79                      65.0 (72.2)     55.0 (61.1)   \n",
       "                    80+                        55.0 (78.6)     50.0 (71.4)   \n",
       "sex                 F                         225.0 (73.8)    215.0 (70.5)   \n",
       "                    M                         250.0 (72.5)    240.0 (69.6)   \n",
       "region              East Midlands              40.0 (72.7)     40.0 (72.7)   \n",
       "                    East of England            25.0 (71.4)     20.0 (57.1)   \n",
       "                    London                     55.0 (73.3)     55.0 (73.3)   \n",
       "                    North East                 35.0 (77.8)     30.0 (66.7)   \n",
       "                    North West                 25.0 (83.3)     15.0 (50.0)   \n",
       "                    South East                 70.0 (77.8)     70.0 (77.8)   \n",
       "                    West Midlands              35.0 (70.0)     35.0 (70.0)   \n",
       "                    Yorkshire and the Humber   30.0 (85.7)     25.0 (71.4)   \n",
       "imd                 1 Most deprived           110.0 (75.9)     95.0 (65.5)   \n",
       "                    2                          90.0 (69.2)     85.0 (65.4)   \n",
       "                    3                         100.0 (76.9)     90.0 (69.2)   \n",
       "                    4                          90.0 (72.0)     95.0 (76.0)   \n",
       "                    5 Least deprived           95.0 (79.2)     85.0 (70.8)   \n",
       "                    Unknown                          - (-)           - (-)   \n",
       "dementia            False                     475.0 (73.6)    450.0 (69.8)   \n",
       "                    True                       5.0 (100.0)           - (-)   \n",
       "diabetes            False                     475.0 (74.2)    450.0 (70.3)   \n",
       "                    True                             - (-)      5.0 (50.0)   \n",
       "hypertension        False                     475.0 (73.6)    450.0 (69.8)   \n",
       "                    True                             - (-)           - (-)   \n",
       "learning_disability False                     475.0 (74.2)    450.0 (70.3)   \n",
       "                    True                        5.0 (50.0)           - (-)   \n",
       "\n",
       "                                             ethnicity primis 5    all filled  \\\n",
       "group               subgroup                                                    \n",
       "all                 with records                   490.0 (75.4)  265.0 (40.8)   \n",
       "age_band            0-19                            65.0 (72.2)   35.0 (38.9)   \n",
       "                    20-29                           65.0 (76.5)   35.0 (41.2)   \n",
       "                    30-39                           55.0 (73.3)   25.0 (33.3)   \n",
       "                    40-49                           55.0 (68.8)   35.0 (43.8)   \n",
       "                    50-59                           55.0 (73.3)   25.0 (33.3)   \n",
       "                    60-69                           75.0 (83.3)   45.0 (50.0)   \n",
       "                    70-79                           65.0 (72.2)   30.0 (33.3)   \n",
       "                    80+                             55.0 (78.6)   35.0 (50.0)   \n",
       "sex                 F                              225.0 (73.8)  115.0 (37.7)   \n",
       "                    M                              260.0 (75.4)  150.0 (43.5)   \n",
       "region              East Midlands                   40.0 (72.7)   25.0 (45.5)   \n",
       "                    East of England                 30.0 (85.7)   15.0 (42.9)   \n",
       "                    London                          50.0 (66.7)   30.0 (40.0)   \n",
       "                    North East                      40.0 (88.9)   25.0 (55.6)   \n",
       "                    North West                      25.0 (83.3)   10.0 (33.3)   \n",
       "                    South East                      70.0 (77.8)   45.0 (50.0)   \n",
       "                    West Midlands                   30.0 (60.0)   20.0 (40.0)   \n",
       "                    Yorkshire and the Humber        25.0 (71.4)   15.0 (42.9)   \n",
       "imd                 1 Most deprived                105.0 (72.4)   60.0 (41.4)   \n",
       "                    2                              100.0 (76.9)   50.0 (38.5)   \n",
       "                    3                               90.0 (69.2)   45.0 (34.6)   \n",
       "                    4                              100.0 (80.0)   50.0 (40.0)   \n",
       "                    5 Least deprived                95.0 (79.2)   55.0 (45.8)   \n",
       "                    Unknown                               - (-)         - (-)   \n",
       "dementia            False                          480.0 (74.4)  260.0 (40.3)   \n",
       "                    True                            5.0 (100.0)         - (-)   \n",
       "diabetes            False                          480.0 (75.0)  260.0 (40.6)   \n",
       "                    True                           10.0 (100.0)         - (-)   \n",
       "hypertension        False                          480.0 (74.4)  260.0 (40.3)   \n",
       "                    True                            5.0 (100.0)         - (-)   \n",
       "learning_disability False                          480.0 (75.0)  260.0 (40.6)   \n",
       "                    True                           10.0 (100.0)         - (-)   \n",
       "\n",
       "                                             population  \n",
       "group               subgroup                             \n",
       "all                 with records                  650.0  \n",
       "age_band            0-19                           90.0  \n",
       "                    20-29                          85.0  \n",
       "                    30-39                          75.0  \n",
       "                    40-49                          80.0  \n",
       "                    50-59                          75.0  \n",
       "                    60-69                          90.0  \n",
       "                    70-79                          90.0  \n",
       "                    80+                            70.0  \n",
       "sex                 F                             305.0  \n",
       "                    M                             345.0  \n",
       "region              East Midlands                  55.0  \n",
       "                    East of England                35.0  \n",
       "                    London                         75.0  \n",
       "                    North East                     45.0  \n",
       "                    North West                     30.0  \n",
       "                    South East                     90.0  \n",
       "                    West Midlands                  50.0  \n",
       "                    Yorkshire and the Humber       35.0  \n",
       "imd                 1 Most deprived               145.0  \n",
       "                    2                             130.0  \n",
       "                    3                             130.0  \n",
       "                    4                             125.0  \n",
       "                    5 Least deprived              120.0  \n",
       "                    Unknown                           -  \n",
       "dementia            False                         645.0  \n",
       "                    True                            5.0  \n",
       "diabetes            False                         640.0  \n",
       "                    True                           10.0  \n",
       "hypertension        False                         645.0  \n",
       "                    True                            5.0  \n",
       "learning_disability False                         640.0  \n",
       "                    True                           10.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean = import_clean(input_path, definitions, other_vars_combined, demographic_covariates, \n",
    "                    clinical_covariates, null, date_min, date_max, \n",
    "                    time_delta, output_path, code_dict, dates)\n",
    "\n",
    "# latest_common_comparison(df_clean, definitions, other_vars_combined, output_path,code_dict)\n",
    "# print(code_dict)\n",
    "from itertools import product\n",
    "new_filt = dict(filter(lambda val: val in definitions, code_dict.items()))\n",
    "lowerlist=[x.lower() for x in (list(code_dict['ethnicity_5'].values()))]\n",
    "patient_counts(df_clean, definitions, demographic_covariates, clinical_covariates, output_path)\n",
    "# print(new_filt)\n",
    "# for x in code_dict['ethnicity_5']:\n",
    "#     print(code_dict['ethnicity_5'][x].lower())\n",
    "# state_change(df_clean, definitions, other_vars_combined, output_path,code_dict)\n",
    "# #state_change(df_clean, definitions, other_vars_combined, output_path)\n",
    "# latest_common_comparison(df_clean, definitions, other_vars_combined, output_path,code_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd7deed66f04c8f1cb07fbe5b34625baefe8e7d41af3b53c9662fc8ba397df1b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
